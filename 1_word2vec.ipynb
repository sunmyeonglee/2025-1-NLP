{"cells":[{"cell_type":"markdown","metadata":{"id":"WXQsDEWukzqU"},"source":["# Word2Vec Implementation from Scratch\n","\n","This notebook demonstrates how to implement the Word2Vec algorithm from scratch using PyTorch. We'll use the first Harry Potter book as our corpus to train word embeddings.\n"]},{"cell_type":"markdown","metadata":{"id":"bnl2eGwPkzqW"},"source":["## 1. Setting Up the Environment\n","\n","First, we need to import the necessary libraries:\n","- `torch` and `torch.nn` for tensor operations and neural network functionality\n","- `string` for string manipulations (removing punctuation)\n"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"L9BA5Lg2QRMr","executionInfo":{"status":"ok","timestamp":1742275334605,"user_tz":-540,"elapsed":9605,"user":{"displayName":"SunMyeong Lee","userId":"02273362555347354442"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import string\n"]},{"cell_type":"markdown","metadata":{"id":"0lUq3GHEkzqX"},"source":["## 2. Getting the Text Data\n","\n","We'll download the first Harry Potter book to use as our corpus."]},{"cell_type":"code","execution_count":2,"metadata":{"id":"tEaaz_s0QRMs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1742275335486,"user_tz":-540,"elapsed":871,"user":{"displayName":"SunMyeong Lee","userId":"02273362555347354442"}},"outputId":"3174fa46-5553-4a8f-b41e-7e0cff48bcb7"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2025-03-18 05:22:14--  https://raw.githubusercontent.com/amephraim/nlp/master/texts/J.%20K.%20Rowling%20-%20Harry%20Potter%201%20-%20Sorcerer's%20Stone.txt\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 439742 (429K) [text/plain]\n","Saving to: ‘J. K. Rowling - Harry Potter 1 - Sorcerer's Stone.txt’\n","\n","J. K. Rowling - Har 100%[===================>] 429.44K  2.11MB/s    in 0.2s    \n","\n","2025-03-18 05:22:15 (2.11 MB/s) - ‘J. K. Rowling - Harry Potter 1 - Sorcerer's Stone.txt’ saved [439742/439742]\n","\n"]}],"source":["!wget \"https://raw.githubusercontent.com/amephraim/nlp/master/texts/J.%20K.%20Rowling%20-%20Harry%20Potter%201%20-%20Sorcerer's%20Stone.txt\"\n"]},{"cell_type":"markdown","metadata":{"id":"RZzrT5mfkzqX"},"source":["## 3. Text Preprocessing\n","\n","Before we can use the text data, we need to preprocess it:\n","- Remove punctuation\n","- Convert text to lowercase\n","- Split text into tokens (words)\n","\n","This function will help us clean and tokenize the text."]},{"cell_type":"code","execution_count":3,"metadata":{"id":"CUsXJYlIQRMs","executionInfo":{"status":"ok","timestamp":1742275335508,"user_tz":-540,"elapsed":21,"user":{"displayName":"SunMyeong Lee","userId":"02273362555347354442"}}},"outputs":[],"source":["def remove_punctuation(x):\n","  return x.translate(''.maketrans('', '', string.punctuation))\n","\n","def make_tokenized_corpus(corpus):\n","  out= [ [y.lower() for y in remove_punctuation(sentence).split(' ') if y] for sentence in corpus]\n","  return [x for x in out if x!=[]]\n"]},{"cell_type":"markdown","metadata":{"id":"Cw404pzckzqY"},"source":["## 4. Loading and Formatting the Text\n","\n","Now we'll load the text file, replace some special characters, and split the text into sentences.\n"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"Ry1o-F-bQRMs","executionInfo":{"status":"ok","timestamp":1742275335576,"user_tz":-540,"elapsed":66,"user":{"displayName":"SunMyeong Lee","userId":"02273362555347354442"}}},"outputs":[],"source":["with open(\"J. K. Rowling - Harry Potter 1 - Sorcerer's Stone.txt\", 'r') as f:\n","  strings = f.readlines()\n","list_of_sentences = \"\".join(strings).replace('\\n', ' ').replace('Mr.', 'mr').replace('Mrs.', 'mrs').split('. ')"]},{"cell_type":"markdown","metadata":{"id":"ORi6DgWjkzqY"},"source":["Let's tokenize the text using our preprocessing function `make_tokenized_corpus`:"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"Ne-pUaxSQRMs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1742275335692,"user_tz":-540,"elapsed":4,"user":{"displayName":"SunMyeong Lee","userId":"02273362555347354442"}},"outputId":"c304e790-902e-4ddd-d024-33f62cf9fa30"},"outputs":[{"output_type":"stream","name":"stdout","text":["Harry Potter and the Sorcerer's Stone   CHAPTER ONE  THE BOY WHO LIVED  mr and mrs Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal, thank you very much\n","They were the last people you'd expect to be involved in anything strange or mysterious, because they just didn't hold with such nonsense\n"," mr Dursley was the director of a firm called Grunnings, which made drills\n","He was a big, beefy man with hardly any neck, although he did have a very large mustache\n","mrs Dursley was thin and blonde and had nearly twice the usual amount of neck, which came in very useful as she spent so much of her time craning over garden fences, spying on the neighbors\n","The Dursleys had a small son called Dudley and in their opinion there was no finer boy anywhere\n"," The Dursleys had everything they wanted, but they also had a secret, and their greatest fear was that somebody would discover it\n","They didn't think they could bear it if anyone found out about the Potters\n","mrs Potter was mrs Dursley's sister, but they hadn't met for several years; in fact, mrs Dursley pretended she didn't have a sister, because her sister and her good-for-nothing husband were as unDursleyish as it was possible to be\n","The Dursleys shuddered to think what the neighbors would say if the Potters arrived in the street\n"]}],"source":["# Corpus is a list of list of strings (words)\n","\n","for sentence in list_of_sentences[:10]:\n","  print(sentence)"]},{"cell_type":"code","source":["corpus = make_tokenized_corpus(list_of_sentences)\n","\n","type(corpus), type(corpus[0]), type(corpus[0][0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FpiLxjWqn-wG","executionInfo":{"status":"ok","timestamp":1742275336026,"user_tz":-540,"elapsed":334,"user":{"displayName":"SunMyeong Lee","userId":"02273362555347354442"}},"outputId":"407ec1bb-d383-4058-dfb6-c4639695c67b"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(list, list, str)"]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"tUqwddUAkzqY"},"source":["## 5. Creating Context Word Pairs\n","\n","A key concept in Word2Vec is learning from context. We need to create pairs of words that appear near each other in the text. We'll use a sliding window approach to create these pairs.\n","\n","For example, with the window size of 2, for the word \"to\" in the sentence \"they were the last people youd expect to be involved...\", we would create pairs with:\n","- (\"to\", \"expect\")\n","- (\"to\", \"be\")\n","- (\"to\", \"involved\")\n","- (\"to\", \"in\")\n","\n","These pairs will be our training data."]},{"cell_type":"code","execution_count":7,"metadata":{"id":"-UxJwTAacWfP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1742275336828,"user_tz":-540,"elapsed":805,"user":{"displayName":"SunMyeong Lee","userId":"02273362555347354442"}},"outputId":"514c6ee7-8a36-494d-d1cc-3524939b1d63"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 4682/4682 [00:00<00:00, 5503.03it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Length of word_pairs is 282372\n","First 5 example of word_pairs is [('harry', 'potter'), ('harry', 'and'), ('potter', 'harry'), ('potter', 'and'), ('potter', 'the')]\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["from tqdm import tqdm\n","\n","sample_sentence = ['they', 'were', 'the', 'last', 'people', 'youd', 'expect', 'to', 'be', 'involved', 'in', 'anything', 'strange', 'or', 'mysterious', 'because', 'they', 'just', 'didnt', 'hold', 'with', 'such', 'nonsense']\n","\n","word_pairs = []\n","window_size = 2\n","\n","for sample_sentence in tqdm(corpus):\n","  for cur_idx, center_word in enumerate(sample_sentence):\n","    window_begin = max(cur_idx - window_size, 0)\n","    window_end = min(cur_idx + window_size + 1, len(sample_sentence))\n","    # for context_word in sample_sentence[window_begin:window_end]:\n","    #   # if center_word == context_word: continue\n","    #   word_pairs.append( (center_word, context_word))\n","    for j in range(window_begin, window_end):\n","      if cur_idx == j: continue\n","      word_pairs.append( (center_word, sample_sentence[j]))\n","\n","print(f\"\\nLength of word_pairs is {len(word_pairs)}\")\n","print(f\"First 5 example of word_pairs is {word_pairs[:5]}\")"]},{"cell_type":"markdown","metadata":{"id":"aELIKMG2kzqZ"},"source":["## 6. Building the Vocabulary\n","\n","To work with word vectors, we need to create a vocabulary that maps each unique word to an index. We'll also filter out rare words that appear less than a certain number of times in the corpus.\n","\n","### 6.1 Collecting All Words\n","\n","First, let's collect all words in our corpus:\n"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"htinJiMPkkRE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1742275337088,"user_tz":-540,"elapsed":257,"user":{"displayName":"SunMyeong Lee","userId":"02273362555347354442"}},"outputId":"bd65f003-08aa-4ffc-d777-483501d934b1"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["77597"]},"metadata":{},"execution_count":8}],"source":["# we have to make vocabulary\n","sentence = corpus[0]\n","entire_words = []\n","\n","for sentence in corpus:\n","  for word in sentence:\n","    entire_words.append(word)\n","\n","len(entire_words)"]},{"cell_type":"markdown","metadata":{"id":"ACx84hwWkzqZ"},"source":["\n","### 6.2 Finding Unique Words\n","\n","Now, let's find the unique words in our corpus:"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"ERBFCjeslgDe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1742275337107,"user_tz":-540,"elapsed":17,"user":{"displayName":"SunMyeong Lee","userId":"02273362555347354442"}},"outputId":"ad4a0969-be86-47f4-d80e-207b741bd830"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["6038"]},"metadata":{},"execution_count":9}],"source":["# we have to get the \"unique\" item among total words\n","\n","vocab_set = set(entire_words)\n","len(vocab_set)\n","\n","unique_words = list(vocab_set)\n","len(unique_words)"]},{"cell_type":"markdown","metadata":{"id":"F2gGyVUqkzqZ"},"source":["### 6.3 Converting to a List and Sorting\n","\n","We'll convert the set of unique words to a sorted list:"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"fDJNrHdhl_dk","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1742275337108,"user_tz":-540,"elapsed":11,"user":{"displayName":"SunMyeong Lee","userId":"02273362555347354442"}},"outputId":"985a8bc2-76c8-4ecf-944d-eee5bb899fb9"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\the'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":10}],"source":["# vocab_set[0] # set is not subscriptable because it has no order\n","\n","unique_words = sorted(list(unique_words))\n","unique_words[0]"]},{"cell_type":"markdown","metadata":{"id":"coTM3X2ikzqZ"},"source":["### 6.4 Filtering by Frequency\n","\n","Now, let's filter out rare words that occur less than a specified number of times:\n","- We can use the `Counter` class from the `collections` module to count the frequency of each word in the corpus.\n","- Caution on `alist.sort()` will return `None`."]},{"cell_type":"code","execution_count":11,"metadata":{"id":"wOkBSjrkmNE4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1742275337111,"user_tz":-540,"elapsed":5,"user":{"displayName":"SunMyeong Lee","userId":"02273362555347354442"}},"outputId":"3abb5dc0-94a6-4e9b-d188-f267b4b71efa"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['a',\n"," 'able',\n"," 'abou',\n"," 'about',\n"," 'above',\n"," 'across',\n"," 'added',\n"," 'afford',\n"," 'afraid',\n"," 'after']"]},"metadata":{},"execution_count":11}],"source":["# how can we filter the vocab by its frequency?\n","filtered_vocab = None\n","# you can use word counter as dictionary\n","# In python dictionary, dict.keys() gives keys, and dict.values() give values,\n","# dict.items() give (key, value)\n","\n","from collections import Counter\n","word_counter = Counter(entire_words)\n","word_counter.most_common(10)\n","word_counter['harry']\n","\n","threshold = 5\n","filtered_vocab = []\n","for key, value in word_counter.items():\n","  if value > threshold:\n","    filtered_vocab.append(key)\n","\n","filtered_vocab.sort()\n","filtered_vocab[:10]"]},{"cell_type":"markdown","metadata":{"id":"u3UPyaOQkzqZ"},"source":["## 7. Filtering Word Pairs\n","\n","Now that we have our filtered vocabulary, we need to filter our word pairs to only include words that are in our vocabulary:"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"XUS6U7y7opUp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1742275348108,"user_tz":-540,"elapsed":10997,"user":{"displayName":"SunMyeong Lee","userId":"02273362555347354442"}},"outputId":"0d43e1e2-a702-41ce-96b6-2dbd9381cb49"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 282372/282372 [00:11<00:00, 25466.78it/s]\n"]}],"source":["# Filter the word_pairs using the vocab\n","# word_pairs, filtered_vocab\n","# word_pairs is a list of [word_a, word_b]\n","\n","filtered_word_pairs = []\n","vocab_set = set(filtered_vocab)\n","\n","for pair in tqdm(word_pairs):\n","  a, b = pair\n","  if a in filtered_vocab and b in filtered_vocab:\n","    filtered_word_pairs.append(pair)"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"U-o4UucOrcem","executionInfo":{"status":"ok","timestamp":1742275463049,"user_tz":-540,"elapsed":8,"user":{"displayName":"SunMyeong Lee","userId":"02273362555347354442"}}},"outputs":[],"source":["# implement same algorithm with list comprehension\n","\n","filtered_word_pairs = [pair for pair in word_pairs if pair[0] in vocab_set and pair[1] in vocab_set]"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"Uz_8ch59ps_P","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1742275348223,"user_tz":-540,"elapsed":15,"user":{"displayName":"SunMyeong Lee","userId":"02273362555347354442"}},"outputId":"762d8268-1952-41dd-eca5-6015681a3331"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(226846, 282372)"]},"metadata":{},"execution_count":14}],"source":["len(filtered_word_pairs), len(word_pairs)"]},{"cell_type":"markdown","metadata":{"id":"HrdAwmBykzqZ"},"source":["## 8. Converting Words to Indices\n","\n","For efficiency, we'll convert our words to indices according to their position in our vocabulary:"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"VFJfhOznqyi-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1742275348223,"user_tz":-540,"elapsed":13,"user":{"displayName":"SunMyeong Lee","userId":"02273362555347354442"}},"outputId":"83cd276e-c1e8-4f47-f717-77592279c5a4"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["527"]},"metadata":{},"execution_count":15}],"source":["# convert word into index of vocab\n","# filtered_vocab.index('happily')\n","filtered_vocab.index('harry')"]},{"cell_type":"markdown","metadata":{"id":"g1Iv7dxRkzqZ"},"source":["This is inefficient because `list.index()` has to scan the list every time. Let's use a dictionary for faster lookups:"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"2D8n16VitIHP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1742275385836,"user_tz":-540,"elapsed":108,"user":{"displayName":"SunMyeong Lee","userId":"02273362555347354442"}},"outputId":"2cddbd2c-8b92-4730-d417-1fb6cdf8c97a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["527"]},"metadata":{},"execution_count":17}],"source":["# we can make it faster\n","# use dictionary to find the index of string\n","word2idx = dict()\n","for idx, word in enumerate(filtered_vocab):\n","  word2idx[word] = idx\n","\n","word2idx['harry']"]},{"cell_type":"markdown","metadata":{"id":"BQr7ddPnkzqa"},"source":["Now, let's convert our word pairs to index pairs more efficiently:"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"utXemuOgt8-o","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1742275659505,"user_tz":-540,"elapsed":176,"user":{"displayName":"SunMyeong Lee","userId":"02273362555347354442"}},"outputId":"0e55473f-7ef5-4b4d-ae3b-12d529e79839"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(527, 953)"]},"metadata":{},"execution_count":25}],"source":["index_pairs = [(word2idx[pair[0]], word2idx[pair[1]]) for pair in filtered_word_pairs]\n","index_pairs[0]"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"KgQ_oSNGuZAd","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1742275692747,"user_tz":-540,"elapsed":25,"user":{"displayName":"SunMyeong Lee","userId":"02273362555347354442"}},"outputId":"0419737a-8f06-4004-c30c-9b8f76817eaf"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'harry'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":26}],"source":["# Why we don't need idx2tok?\n","\n","filtered_vocab[527]"]},{"cell_type":"markdown","metadata":{"id":"2igwOubJkzqa"},"source":["## 9. Creating Initial Word Vectors\n","\n","Now we'll create random vectors for each word in our vocabulary. These vectors will be adjusted during training:\n","- We can use `torch.randn` to create random vectors that follow normal distribution."]},{"cell_type":"code","execution_count":42,"metadata":{"id":"ygV93qzDu4Ls","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1742276185773,"user_tz":-540,"elapsed":25,"user":{"displayName":"SunMyeong Lee","userId":"02273362555347354442"}},"outputId":"3d358dbf-eb67-4a5d-b668-f663653de0f5"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[-0.0726, -0.1422, -0.1386,  ...,  0.1413,  0.0136,  0.0928],\n","        [ 0.0387,  0.0684, -0.0188,  ...,  0.0316,  0.0317, -0.1438],\n","        [-0.0268, -0.0022, -0.0271,  ...,  0.0574,  0.0139,  0.1331],\n","        ...,\n","        [ 0.0339,  0.0197, -0.1009,  ...,  0.0948, -0.0939, -0.1432],\n","        [ 0.0111, -0.0825,  0.0333,  ..., -0.0428,  0.0658, -0.0670],\n","        [-0.1141, -0.0913,  0.0875,  ..., -0.0803,  0.1407,  0.0963]])"]},"metadata":{},"execution_count":42}],"source":["# we have to make random vectors for each word in the vocab\n","# we also have to decide the dimension of the vector\n","\n","dim = 100\n","vocab_size = len(filtered_vocab)\n","\n","word_vectors = torch.randn(vocab_size, dim) / 10\n","word_vectors"]},{"cell_type":"code","execution_count":41,"metadata":{"id":"vmZcT53rvwW2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1742276181640,"user_tz":-540,"elapsed":14,"user":{"displayName":"SunMyeong Lee","userId":"02273362555347354442"}},"outputId":"d1612c68-707a-4fce-8ea2-238b8263fc80"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([ 0.1187, -0.1411, -0.0562, -0.0851,  0.0260,  0.1393, -0.0842, -0.0380,\n","         0.0117,  0.1649,  0.0039,  0.0540, -0.0452, -0.0119,  0.0271, -0.1264,\n","        -0.0613, -0.0285,  0.1139, -0.0633, -0.0860, -0.1596, -0.0210, -0.1657,\n","         0.0979,  0.0501, -0.0266, -0.0172,  0.1560, -0.2785, -0.0872, -0.0325,\n","        -0.1014,  0.0164, -0.0384,  0.0729, -0.0312, -0.1175, -0.1621,  0.0580,\n","         0.0072,  0.0178, -0.0772, -0.1279, -0.1002, -0.0670,  0.0744, -0.1892,\n","        -0.2057, -0.0350, -0.0114, -0.0350,  0.0042,  0.1155, -0.0968, -0.1576,\n","         0.0171, -0.0546,  0.0132,  0.0462,  0.1590,  0.0126, -0.0596,  0.0968,\n","        -0.0713,  0.0703, -0.0761,  0.0783,  0.0505, -0.0598,  0.0283,  0.0273,\n","        -0.1183, -0.0937, -0.0943, -0.0072,  0.0384,  0.0667,  0.0424, -0.0901,\n","         0.0037, -0.0839,  0.0018, -0.0168, -0.0254,  0.0090, -0.0418,  0.1053,\n","        -0.0848,  0.0636, -0.0892,  0.0880, -0.0231,  0.1676, -0.0815, -0.2471,\n","        -0.0098,  0.0412,  0.3541,  0.0319])"]},"metadata":{},"execution_count":41}],"source":["# what is the vector for harry?\n","word_vectors[word2idx['harry']]"]},{"cell_type":"markdown","metadata":{"id":"kUCg10rHkzqa"},"source":["## 10. Understanding Word Relationships with Dot Products\n","\n","The core of Word2Vec is using dot products to measure relationships between words. Let's explore this concept:"]},{"cell_type":"code","execution_count":56,"metadata":{"id":"J4bEKFvUxbVv","executionInfo":{"status":"ok","timestamp":1742276523728,"user_tz":-540,"elapsed":3,"user":{"displayName":"SunMyeong Lee","userId":"02273362555347354442"}}},"outputs":[],"source":["torch.set_printoptions(sci_mode=False) # Do this to avoid scientific notation"]},{"cell_type":"markdown","metadata":{"id":"VaCMWmD6Suhy"},"source":["## Dot Product\n","- Assume we have two vectors $a$ and $b$.\n","  - $a = [a_1, a_2, a_3, a_4, ..., a_n]$\n","  - $b = [b_1, b_2, b_3, b_4, ..., b_n]$\n","- $a \\cdot b$ = $\\sum _{i=1}^n a_ib_i$  = $a_1b_1 + a_2b_2 + a_3b_3 + a_4b_4 + ... + a_nb_n$\n","\n","Let's calculate the dot product between \"harry\" and \"potter\":\n"]},{"cell_type":"code","execution_count":57,"metadata":{"id":"gsse-jUrw6c2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1742276525024,"user_tz":-540,"elapsed":5,"user":{"displayName":"SunMyeong Lee","userId":"02273362555347354442"}},"outputId":"517eacd5-539c-446c-cb59-3a21c611d099"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(-0.1254)"]},"metadata":{},"execution_count":57}],"source":["# calculate P(potter|harry)\n","harry = word_vectors[word2idx['harry']]\n","potter = word_vectors[word2idx['potter']]\n","dot_product_value_between_potter_harry = sum(harry * potter)\n","dot_product_value_between_potter_harry"]},{"cell_type":"code","execution_count":58,"metadata":{"id":"wZrLEY36yBNZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1742276527476,"user_tz":-540,"elapsed":911,"user":{"displayName":"SunMyeong Lee","userId":"02273362555347354442"}},"outputId":"287e5ed8-8baa-4f41-b75c-b7a6cad5eae5"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'a': tensor(0.0147),\n"," 'able': tensor(0.1449),\n"," 'abou': tensor(-0.0150),\n"," 'about': tensor(-0.0696),\n"," 'above': tensor(-0.2321),\n"," 'across': tensor(-0.0507),\n"," 'added': tensor(0.1369),\n"," 'afford': tensor(0.1165),\n"," 'afraid': tensor(-0.1324),\n"," 'after': tensor(0.2381),\n"," 'afternoon': tensor(0.2201),\n"," 'again': tensor(0.0086),\n"," 'against': tensor(-0.0662),\n"," 'ages': tensor(-0.0308),\n"," 'ago': tensor(0.0238),\n"," 'agreed': tensor(0.0588),\n"," 'ah': tensor(0.2077),\n"," 'ahead': tensor(0.0122),\n"," 'air': tensor(-0.0275),\n"," 'albus': tensor(-0.0412),\n"," 'alive': tensor(0.0134),\n"," 'all': tensor(-0.1455),\n"," 'alley': tensor(0.1873),\n"," 'allowed': tensor(-0.0368),\n"," 'almost': tensor(0.0034),\n"," 'alone': tensor(-0.0948),\n"," 'along': tensor(0.0067),\n"," 'already': tensor(-0.0221),\n"," 'also': tensor(-0.0637),\n"," 'although': tensor(0.0992),\n"," 'always': tensor(-0.0548),\n"," 'am': tensor(-0.0636),\n"," 'an': tensor(0.1376),\n"," 'and': tensor(0.0510),\n"," 'angrily': tensor(-0.0372),\n"," 'angry': tensor(0.2142),\n"," 'another': tensor(-0.0292),\n"," 'answer': tensor(-0.1017),\n"," 'any': tensor(-0.1782),\n"," 'anymore': tensor(0.0321),\n"," 'anyone': tensor(0.1277),\n"," 'anythin': tensor(0.0355),\n"," 'anything': tensor(0.0110),\n"," 'anyway': tensor(0.0140),\n"," 'anywhere': tensor(0.0416),\n"," 'apart': tensor(0.3407),\n"," 'appeared': tensor(0.1478),\n"," 'are': tensor(-0.0081),\n"," 'arent': tensor(0.0893),\n"," 'arm': tensor(-0.1360),\n"," 'armor': tensor(-0.0515),\n"," 'arms': tensor(-0.1266),\n"," 'around': tensor(0.0247),\n"," 'arrived': tensor(-0.0146),\n"," 'arts': tensor(-0.1614),\n"," 'as': tensor(-0.1192),\n"," 'ask': tensor(0.0145),\n"," 'asked': tensor(-0.1356),\n"," 'asking': tensor(-0.0141),\n"," 'asleep': tensor(-0.2794),\n"," 'at': tensor(-0.0863),\n"," 'attention': tensor(0.1490),\n"," 'aunt': tensor(-0.1050),\n"," 'awake': tensor(-0.0422),\n"," 'away': tensor(-0.0394),\n"," 'baby': tensor(0.0646),\n"," 'back': tensor(-0.1860),\n"," 'backward': tensor(0.0364),\n"," 'bacon': tensor(0.0117),\n"," 'bad': tensor(-0.0411),\n"," 'bag': tensor(-0.1051),\n"," 'ball': tensor(0.0643),\n"," 'balls': tensor(0.1101),\n"," 'bane': tensor(-0.0081),\n"," 'barrier': tensor(0.0159),\n"," 'be': tensor(0.1099),\n"," 'beans': tensor(-0.0315),\n"," 'beard': tensor(0.0250),\n"," 'became': tensor(0.0051),\n"," 'because': tensor(-0.0590),\n"," 'become': tensor(0.0962),\n"," 'bed': tensor(0.0076),\n"," 'bedroom': tensor(-0.0723),\n"," 'been': tensor(-0.0853),\n"," 'before': tensor(0.0064),\n"," 'began': tensor(-0.1391),\n"," 'behind': tensor(0.0571),\n"," 'being': tensor(0.0737),\n"," 'believe': tensor(-0.0743),\n"," 'below': tensor(-0.0201),\n"," 'beneath': tensor(0.0254),\n"," 'bent': tensor(-0.0324),\n"," 'best': tensor(-0.0692),\n"," 'bet': tensor(0.0324),\n"," 'better': tensor(0.0973),\n"," 'between': tensor(-0.0625),\n"," 'big': tensor(0.1003),\n"," 'bill': tensor(-0.0468),\n"," 'bin': tensor(-0.1855),\n"," 'binoculars': tensor(-0.0277),\n"," 'birthday': tensor(-0.3384),\n"," 'bit': tensor(-0.1665),\n"," 'black': tensor(-0.0758),\n"," 'blankets': tensor(0.0574),\n"," 'blew': tensor(-0.0645),\n"," 'blood': tensor(0.0586),\n"," 'bloody': tensor(0.0755),\n"," 'bludger': tensor(-0.2078),\n"," 'bludgers': tensor(0.0409),\n"," 'blue': tensor(-0.0307),\n"," 'board': tensor(-0.0596),\n"," 'boat': tensor(-0.1278),\n"," 'boats': tensor(-0.0749),\n"," 'body': tensor(0.1754),\n"," 'book': tensor(0.0929),\n"," 'books': tensor(0.0045),\n"," 'both': tensor(0.2165),\n"," 'bottle': tensor(-0.0160),\n"," 'bottles': tensor(-0.0076),\n"," 'bottom': tensor(-0.1087),\n"," 'bought': tensor(0.0054),\n"," 'bowed': tensor(-0.0763),\n"," 'box': tensor(0.1022),\n"," 'boy': tensor(-0.0021),\n"," 'boys': tensor(0.0262),\n"," 'branches': tensor(0.1315),\n"," 'brave': tensor(-0.0751),\n"," 'break': tensor(-0.0212),\n"," 'breakfast': tensor(-0.0183),\n"," 'breaking': tensor(0.0314),\n"," 'breath': tensor(0.2729),\n"," 'breathing': tensor(-0.1160),\n"," 'bright': tensor(-0.0891),\n"," 'brilliant': tensor(-0.0491),\n"," 'broke': tensor(-0.0221),\n"," 'broken': tensor(0.0510),\n"," 'broom': tensor(-0.0471),\n"," 'brooms': tensor(-0.0409),\n"," 'broomstick': tensor(0.1778),\n"," 'broomsticks': tensor(-0.1134),\n"," 'brother': tensor(0.0731),\n"," 'brothers': tensor(0.2935),\n"," 'brought': tensor(0.1869),\n"," 'brown': tensor(0.0552),\n"," 'burst': tensor(0.0326),\n"," 'business': tensor(0.0061),\n"," 'busy': tensor(-0.0501),\n"," 'but': tensor(0.0799),\n"," 'buy': tensor(-0.1797),\n"," 'by': tensor(-0.0603),\n"," 'cake': tensor(0.0953),\n"," 'cakes': tensor(-0.1394),\n"," 'call': tensor(-0.1027),\n"," 'called': tensor(0.0864),\n"," 'came': tensor(-0.1983),\n"," 'can': tensor(0.1364),\n"," 'cant': tensor(0.0193),\n"," 'car': tensor(0.0743),\n"," 'card': tensor(-0.0844),\n"," 'care': tensor(0.0444),\n"," 'careful': tensor(0.0036),\n"," 'carefully': tensor(0.0219),\n"," 'carried': tensor(-0.1878),\n"," 'carrying': tensor(0.1695),\n"," 'cart': tensor(-0.1288),\n"," 'case': tensor(0.0979),\n"," 'castle': tensor(0.0814),\n"," 'cat': tensor(-0.1306),\n"," 'catch': tensor(0.0318),\n"," 'cats': tensor(0.0708),\n"," 'caught': tensor(-0.0261),\n"," 'cauldron': tensor(0.0547),\n"," 'cause': tensor(0.0549),\n"," 'ceiling': tensor(0.1221),\n"," 'centaur': tensor(-0.0172),\n"," 'certainly': tensor(0.0843),\n"," 'chair': tensor(0.0272),\n"," 'chamber': tensor(0.0180),\n"," 'chance': tensor(0.0987),\n"," 'change': tensor(0.0844),\n"," 'changed': tensor(-0.2242),\n"," 'chapter': tensor(0.0866),\n"," 'charlie': tensor(-0.0159),\n"," 'charlies': tensor(0.0480),\n"," 'charms': tensor(-0.0018),\n"," 'chasers': tensor(0.1933),\n"," 'cheer': tensor(0.0010),\n"," 'cheering': tensor(-0.1837),\n"," 'cheers': tensor(0.0405),\n"," 'chess': tensor(0.0334),\n"," 'chessmen': tensor(0.0053),\n"," 'chest': tensor(-0.0331),\n"," 'chocolate': tensor(0.1237),\n"," 'christmas': tensor(-0.0388),\n"," 'chuckled': tensor(-0.0291),\n"," 'clambered': tensor(-0.1291),\n"," 'clapped': tensor(0.1648),\n"," 'class': tensor(-0.1140),\n"," 'classes': tensor(0.1034),\n"," 'classroom': tensor(0.1598),\n"," 'clean': tensor(0.0482),\n"," 'clear': tensor(0.1545),\n"," 'cleared': tensor(0.0132),\n"," 'clearing': tensor(0.0330),\n"," 'clearly': tensor(-0.2671),\n"," 'clicked': tensor(0.2882),\n"," 'climbed': tensor(-0.0364),\n"," 'cloak': tensor(-0.1092),\n"," 'close': tensor(0.0417),\n"," 'closer': tensor(-0.0153),\n"," 'clothes': tensor(0.1593),\n"," 'club': tensor(-0.0571),\n"," 'clutching': tensor(-0.1088),\n"," 'coat': tensor(0.0106),\n"," 'cold': tensor(0.3836),\n"," 'come': tensor(0.0012),\n"," 'coming': tensor(0.1669),\n"," 'common': tensor(-0.0938),\n"," 'compartment': tensor(0.1149),\n"," 'completely': tensor(-0.0611),\n"," 'computer': tensor(-0.0308),\n"," 'control': tensor(0.0511),\n"," 'corner': tensor(0.0073),\n"," 'corridor': tensor(-0.0696),\n"," 'corridors': tensor(0.0459),\n"," 'could': tensor(-0.2434),\n"," 'couldnt': tensor(0.0915),\n"," 'couple': tensor(-0.0647),\n"," 'courage': tensor(0.1860),\n"," 'course': tensor(-0.1079),\n"," 'covered': tensor(0.0457),\n"," 'crabbe': tensor(-0.1180),\n"," 'crack': tensor(-0.2293),\n"," 'crash': tensor(-0.0881),\n"," 'crate': tensor(0.0298),\n"," 'crept': tensor(-0.1131),\n"," 'cried': tensor(0.0185),\n"," 'cross': tensor(-0.0589),\n"," 'crossed': tensor(0.2364),\n"," 'crowd': tensor(-0.0103),\n"," 'cry': tensor(-0.0796),\n"," 'crying': tensor(-0.2357),\n"," 'cup': tensor(0.0473),\n"," 'cupboard': tensor(-0.0735),\n"," 'curious': tensor(-0.0490),\n"," 'curse': tensor(0.0158),\n"," 'cut': tensor(-0.1781),\n"," 'dad': tensor(0.1449),\n"," 'damp': tensor(0.0870),\n"," 'dangerous': tensor(0.2094),\n"," 'dare': tensor(-0.0816),\n"," 'dark': tensor(0.2120),\n"," 'darkly': tensor(0.0331),\n"," 'darkness': tensor(-0.1027),\n"," 'day': tensor(-0.1542),\n"," 'days': tensor(-0.1757),\n"," 'dead': tensor(-0.0255),\n"," 'dean': tensor(-0.0505),\n"," 'dear': tensor(-0.0206),\n"," 'death': tensor(0.0660),\n"," 'decided': tensor(0.0034),\n"," 'deep': tensor(0.1463),\n"," 'delighted': tensor(-0.0343),\n"," 'desk': tensor(-0.0221),\n"," 'desperate': tensor(-0.0459),\n"," 'diagon': tensor(-0.0535),\n"," 'did': tensor(0.1397),\n"," 'didnt': tensor(-0.0073),\n"," 'die': tensor(0.1442),\n"," 'died': tensor(0.0650),\n"," 'difference': tensor(-0.0404),\n"," 'different': tensor(-0.2157),\n"," 'difficult': tensor(0.1415),\n"," 'dinner': tensor(0.0410),\n"," 'direction': tensor(-0.1817),\n"," 'disappeared': tensor(0.0514),\n"," 'dived': tensor(0.2148),\n"," 'do': tensor(-0.3635),\n"," 'does': tensor(0.0659),\n"," 'doesnt': tensor(-0.1280),\n"," 'dog': tensor(-0.1941),\n"," 'dogs': tensor(-0.0574),\n"," 'doing': tensor(0.0152),\n"," 'don': tensor(0.2001),\n"," 'done': tensor(-0.0999),\n"," 'dont': tensor(0.1158),\n"," 'door': tensor(0.0457),\n"," 'doors': tensor(-0.0538),\n"," 'doorway': tensor(0.0130),\n"," 'dormitory': tensor(-0.0933),\n"," 'down': tensor(-0.1085),\n"," 'draco': tensor(0.1017),\n"," 'dragged': tensor(-0.3041),\n"," 'dragon': tensor(0.1510),\n"," 'dragons': tensor(-0.1044),\n"," 'dream': tensor(0.0861),\n"," 'dressed': tensor(-0.0981),\n"," 'drew': tensor(-0.1165),\n"," 'drills': tensor(-0.0466),\n"," 'drink': tensor(0.0268),\n"," 'drive': tensor(0.0133),\n"," 'drop': tensor(0.0699),\n"," 'dropped': tensor(0.0191),\n"," 'drove': tensor(-0.0019),\n"," 'dudley': tensor(-0.0353),\n"," 'dudleys': tensor(-0.1830),\n"," 'dumbledore': tensor(0.0149),\n"," 'dumbledores': tensor(0.1184),\n"," 'dungeons': tensor(0.0645),\n"," 'dunno': tensor(0.1833),\n"," 'during': tensor(-0.0321),\n"," 'dursley': tensor(0.0361),\n"," 'dursleys': tensor(0.0780),\n"," 'each': tensor(0.0429),\n"," 'eagerly': tensor(0.0534),\n"," 'ear': tensor(0.0780),\n"," 'ears': tensor(-0.0522),\n"," 'earth': tensor(0.0918),\n"," 'easily': tensor(0.1375),\n"," 'easy': tensor(-0.0199),\n"," 'eat': tensor(0.1174),\n"," 'eating': tensor(0.0619),\n"," 'edge': tensor(0.1046),\n"," 'egg': tensor(-0.0751),\n"," 'eh': tensor(-0.0388),\n"," 'either': tensor(-0.1457),\n"," 'eleven': tensor(0.1192),\n"," 'else': tensor(0.0514),\n"," 'em': tensor(-0.0602),\n"," 'empty': tensor(0.0290),\n"," 'end': tensor(0.1081),\n"," 'enough': tensor(-0.0633),\n"," 'entered': tensor(-0.1067),\n"," 'entrance': tensor(0.2001),\n"," 'envelope': tensor(0.0367),\n"," 'er': tensor(-0.0446),\n"," 'erised': tensor(0.2418),\n"," 'even': tensor(-0.1812),\n"," 'evening': tensor(-0.0097),\n"," 'ever': tensor(-0.1375),\n"," 'every': tensor(0.0387),\n"," 'everybody': tensor(0.0065),\n"," 'everyone': tensor(-0.1536),\n"," 'everything': tensor(0.1083),\n"," 'everywhere': tensor(-0.0081),\n"," 'evil': tensor(0.1510),\n"," 'exactly': tensor(-0.0398),\n"," 'exam': tensor(0.0604),\n"," 'exams': tensor(-0.0213),\n"," 'excellent': tensor(-0.0144),\n"," 'except': tensor(-0.2487),\n"," 'excitedly': tensor(0.2281),\n"," 'excuse': tensor(0.1109),\n"," 'expect': tensor(0.0772),\n"," 'expected': tensor(-0.0677),\n"," 'expelled': tensor(-0.1172),\n"," 'explain': tensor(0.0302),\n"," 'extra': tensor(0.1566),\n"," 'eye': tensor(0.1438),\n"," 'eyes': tensor(-0.1000),\n"," 'face': tensor(0.1211),\n"," 'faces': tensor(-0.0164),\n"," 'facing': tensor(0.1008),\n"," 'fact': tensor(0.0785),\n"," 'faded': tensor(-0.0774),\n"," 'fall': tensor(0.1355),\n"," 'fallen': tensor(-0.2994),\n"," 'families': tensor(-0.0911),\n"," 'family': tensor(0.1487),\n"," 'famous': tensor(-0.0078),\n"," 'fang': tensor(0.1447),\n"," 'fangs': tensor(-0.0235),\n"," 'far': tensor(-0.1732),\n"," 'fast': tensor(0.0478),\n"," 'fat': tensor(0.0616),\n"," 'father': tensor(0.0055),\n"," 'fathers': tensor(-0.0721),\n"," 'favorite': tensor(-0.0742),\n"," 'fear': tensor(-0.1644),\n"," 'feast': tensor(-0.1965),\n"," 'feather': tensor(0.2203),\n"," 'feel': tensor(-0.1570),\n"," 'feeling': tensor(0.1278),\n"," 'feet': tensor(0.0562),\n"," 'fell': tensor(0.1037),\n"," 'felt': tensor(0.1258),\n"," 'fer': tensor(0.1439),\n"," 'few': tensor(-0.2461),\n"," 'field': tensor(0.0210),\n"," 'fifty': tensor(-0.0277),\n"," 'fight': tensor(0.0965),\n"," 'fighting': tensor(-0.1392),\n"," 'figure': tensor(0.0766),\n"," 'filch': tensor(-0.0941),\n"," 'filled': tensor(-0.0606),\n"," 'finally': tensor(0.0926),\n"," 'find': tensor(-0.0798),\n"," 'finding': tensor(0.2323),\n"," 'fine': tensor(-0.0775),\n"," 'fingers': tensor(-0.0323),\n"," 'finished': tensor(0.0949),\n"," 'finnigan': tensor(-0.0429),\n"," 'fire': tensor(-0.0723),\n"," 'firenze': tensor(-0.0908),\n"," 'firs': tensor(0.1665),\n"," 'first': tensor(-0.1052),\n"," 'five': tensor(0.2125),\n"," 'fixed': tensor(0.1321),\n"," 'flamel': tensor(-0.0153),\n"," 'flames': tensor(-0.0983),\n"," 'flash': tensor(-0.2275),\n"," 'flat': tensor(0.1448),\n"," 'flavor': tensor(0.2263),\n"," 'flew': tensor(-0.1174),\n"," 'flint': tensor(0.0897),\n"," 'flitwick': tensor(0.0691),\n"," 'floating': tensor(0.1875),\n"," 'floor': tensor(-0.0598),\n"," 'fluffy': tensor(-0.0469),\n"," 'flute': tensor(-0.1801),\n"," 'fly': tensor(-0.0482),\n"," 'flying': tensor(0.0664),\n"," 'follow': tensor(-0.1895),\n"," 'followed': tensor(-0.0031),\n"," 'following': tensor(0.1469),\n"," 'food': tensor(-0.0261),\n"," 'foot': tensor(0.1781),\n"," 'footsteps': tensor(-0.1336),\n"," 'for': tensor(-0.0467),\n"," 'forbidden': tensor(0.0738),\n"," 'force': tensor(0.2634),\n"," 'forehead': tensor(0.0641),\n"," 'forest': tensor(0.1262),\n"," 'forget': tensor(0.2162),\n"," 'forgotten': tensor(0.0535),\n"," 'forward': tensor(-0.0391),\n"," 'found': tensor(0.1869),\n"," 'four': tensor(-0.0137),\n"," 'fred': tensor(0.0220),\n"," 'free': tensor(-0.0954),\n"," 'friend': tensor(0.1028),\n"," 'friends': tensor(0.0718),\n"," 'frog': tensor(0.0823),\n"," 'frogs': tensor(0.1457),\n"," 'from': tensor(-0.1037),\n"," 'front': tensor(-0.1287),\n"," 'full': tensor(0.1405),\n"," 'fun': tensor(-0.0981),\n"," 'funny': tensor(-0.2754),\n"," 'furious': tensor(-0.0827),\n"," 'furiously': tensor(0.0175),\n"," 'game': tensor(0.0977),\n"," 'garden': tensor(0.0705),\n"," 'gasped': tensor(0.0418),\n"," 'gave': tensor(0.2191),\n"," 'gently': tensor(0.0525),\n"," 'george': tensor(-0.1004),\n"," 'get': tensor(-0.1466),\n"," 'gets': tensor(0.0013),\n"," 'gettin': tensor(0.0605),\n"," 'getting': tensor(-0.2001),\n"," 'ghost': tensor(-0.0301),\n"," 'ghosts': tensor(-0.0922),\n"," 'giant': tensor(-0.0189),\n"," 'girl': tensor(-0.0215),\n"," 'girls': tensor(-0.1418),\n"," 'give': tensor(-0.0714),\n"," 'given': tensor(-0.0714),\n"," 'giving': tensor(-0.0027),\n"," 'glad': tensor(-0.1909),\n"," 'glass': tensor(-0.1655),\n"," 'glasses': tensor(-0.0989),\n"," 'go': tensor(-0.0827),\n"," 'goal': tensor(-0.1916),\n"," 'goblin': tensor(-0.0514),\n"," 'goblins': tensor(-0.0929),\n"," 'goes': tensor(0.1852),\n"," 'going': tensor(-0.0369),\n"," 'gold': tensor(-0.1484),\n"," 'golden': tensor(0.0275),\n"," 'gone': tensor(0.1635),\n"," 'good': tensor(-0.1252),\n"," 'goodbye': tensor(-0.0988),\n"," 'got': tensor(0.0936),\n"," 'gotta': tensor(-0.0305),\n"," 'gotten': tensor(0.2598),\n"," 'goyle': tensor(-0.2337),\n"," 'grab': tensor(-0.0011),\n"," 'grabbed': tensor(0.0984),\n"," 'granger': tensor(0.1055),\n"," 'grass': tensor(-0.0559),\n"," 'gray': tensor(-0.0222),\n"," 'great': tensor(0.1974),\n"," 'green': tensor(0.0391),\n"," 'grin': tensor(0.1347),\n"," 'gringotts': tensor(0.0891),\n"," 'griphook': tensor(0.0875),\n"," 'ground': tensor(-0.2425),\n"," 'grounds': tensor(-0.0299),\n"," 'growled': tensor(0.0842),\n"," 'grunted': tensor(-0.0192),\n"," 'gryffindor': tensor(-0.0555),\n"," 'gryffindors': tensor(-0.0072),\n"," 'guard': tensor(-0.1259),\n"," 'guarding': tensor(-0.1339),\n"," 'h': tensor(-0.0492),\n"," 'had': tensor(-0.1315),\n"," 'hadnt': tensor(0.0269),\n"," 'hagrid': tensor(-0.1271),\n"," 'hagrids': tensor(-0.0738),\n"," 'hair': tensor(0.0444),\n"," 'half': tensor(0.0642),\n"," 'halfway': tensor(0.1628),\n"," 'hall': tensor(-0.1519),\n"," 'halloween': tensor(-0.0096),\n"," 'hand': tensor(0.0119),\n"," 'handed': tensor(0.1501),\n"," 'handle': tensor(0.0184),\n"," 'hands': tensor(-0.1136),\n"," 'hang': tensor(0.1659),\n"," 'hanging': tensor(-0.0642),\n"," 'happen': tensor(0.0106),\n"," 'happened': tensor(-0.1153),\n"," 'happy': tensor(-0.0427),\n"," 'hard': tensor(0.0611),\n"," 'harder': tensor(-0.1050),\n"," 'hardly': tensor(0.2198),\n"," 'harry': tensor(1.3062),\n"," 'harrys': tensor(-0.0447),\n"," 'has': tensor(0.0584),\n"," 'hasnt': tensor(-0.0617),\n"," 'hat': tensor(0.0160),\n"," 'hate': tensor(-0.1291),\n"," 'hated': tensor(-0.1507),\n"," 'have': tensor(-0.1104),\n"," 'havent': tensor(0.1649),\n"," 'having': tensor(-0.1288),\n"," 'he': tensor(-0.1060),\n"," 'head': tensor(0.0161),\n"," 'headless': tensor(-0.0337),\n"," 'heads': tensor(-0.0126),\n"," 'hear': tensor(-0.0374),\n"," 'heard': tensor(0.1403),\n"," 'heart': tensor(0.2338),\n"," 'heavy': tensor(0.1948),\n"," 'hed': tensor(0.1735),\n"," 'hedwig': tensor(0.0674),\n"," 'held': tensor(-0.2052),\n"," 'hell': tensor(-0.2399),\n"," 'help': tensor(-0.0880),\n"," 'her': tensor(-0.0059),\n"," 'here': tensor(-0.1397),\n"," 'hermione': tensor(0.0261),\n"," 'hermiones': tensor(-0.1069),\n"," 'herself': tensor(-0.3467),\n"," 'hes': tensor(-0.0128),\n"," 'hidden': tensor(0.0679),\n"," 'hide': tensor(-0.1361),\n"," 'hiding': tensor(-0.1842),\n"," 'high': tensor(0.0496),\n"," 'higher': tensor(-0.1833),\n"," 'him': tensor(-0.0495),\n"," 'himself': tensor(0.0509),\n"," 'his': tensor(-0.1404),\n"," 'hissed': tensor(0.0419),\n"," 'history': tensor(0.0263),\n"," 'hit': tensor(-0.0387),\n"," 'hogwarts': tensor(-0.0051),\n"," 'hold': tensor(-0.1687),\n"," 'holding': tensor(0.0521),\n"," 'hole': tensor(-0.0114),\n"," 'holidays': tensor(0.1311),\n"," 'home': tensor(-0.1146),\n"," 'homework': tensor(0.0949),\n"," 'honestly': tensor(0.0093),\n"," 'hooch': tensor(0.0224),\n"," 'hoops': tensor(0.0150),\n"," 'hope': tensor(0.0534),\n"," 'hoping': tensor(0.0111),\n"," 'horrible': tensor(-0.0602),\n"," 'horror': tensor(-0.0733),\n"," 'hospital': tensor(-0.0296),\n"," 'hot': tensor(-0.1093),\n"," 'hour': tensor(0.0128),\n"," 'hours': tensor(-0.0100),\n"," 'house': tensor(0.0558),\n"," 'houses': tensor(0.0323),\n"," 'how': tensor(0.1172),\n"," 'however': tensor(-0.0442),\n"," 'howling': tensor(0.1110),\n"," 'hufflepuff': tensor(-0.0310),\n"," 'huge': tensor(0.0604),\n"," 'human': tensor(-0.0273),\n"," 'hundred': tensor(-0.1126),\n"," 'hundreds': tensor(0.0250),\n"," 'hung': tensor(0.0358),\n"," 'hungry': tensor(0.0712),\n"," 'hurried': tensor(-0.0189),\n"," 'hurry': tensor(0.1687),\n"," 'hurrying': tensor(0.0289),\n"," 'hurt': tensor(-0.1539),\n"," 'hut': tensor(-0.0693),\n"," 'i': tensor(-0.1758),\n"," 'ice': tensor(-0.1742),\n"," 'id': tensor(-0.1779),\n"," 'idea': tensor(0.0242),\n"," 'if': tensor(-0.0356),\n"," 'ignored': tensor(0.1498),\n"," 'ill': tensor(-0.2525),\n"," 'im': tensor(-0.0779),\n"," 'imagine': tensor(0.2004),\n"," 'important': tensor(0.0271),\n"," 'in': tensor(-0.0258),\n"," 'inches': tensor(0.1146),\n"," 'indeed': tensor(0.0575),\n"," 'inside': tensor(0.1598),\n"," 'instead': tensor(-0.0211),\n"," 'interested': tensor(-0.0274),\n"," 'interesting': tensor(-0.1012),\n"," 'into': tensor(-0.1069),\n"," 'invisibility': tensor(0.0262),\n"," 'invisible': tensor(-0.1341),\n"," 'is': tensor(0.0220),\n"," 'isnt': tensor(0.0241),\n"," 'it': tensor(0.0384),\n"," 'itll': tensor(-0.2278),\n"," 'its': tensor(0.0686),\n"," 'itself': tensor(0.0504),\n"," 'ive': tensor(0.0625),\n"," 'jerked': tensor(-0.0047),\n"," 'job': tensor(0.1144),\n"," 'join': tensor(0.0857),\n"," 'joined': tensor(0.0916),\n"," 'joke': tensor(0.0031),\n"," 'jordan': tensor(0.1269),\n"," 'jump': tensor(0.1877),\n"," 'jumped': tensor(-0.0319),\n"," 'jus': tensor(-0.0597),\n"," 'just': tensor(-0.0927),\n"," 'keep': tensor(0.1186),\n"," 'keeper': tensor(-0.0499),\n"," 'keeping': tensor(0.1505),\n"," 'kept': tensor(-0.1027),\n"," 'key': tensor(-0.2185),\n"," 'keys': tensor(0.1471),\n"," 'kicked': tensor(-0.0052),\n"," 'kill': tensor(-0.2555),\n"," 'killed': tensor(-0.0755),\n"," 'kind': tensor(-0.0451),\n"," 'kings': tensor(0.0652),\n"," 'kitchen': tensor(-0.1083),\n"," 'knees': tensor(0.1490),\n"," 'knew': tensor(-0.2059),\n"," 'knight': tensor(0.0474),\n"," 'knock': tensor(-0.0331),\n"," 'knocked': tensor(0.1180),\n"," 'knocking': tensor(0.1764),\n"," 'know': tensor(-0.0718),\n"," 'knowing': tensor(-0.1734),\n"," 'known': tensor(-0.2110),\n"," 'knows': tensor(0.1846),\n"," 'knuts': tensor(-0.0627),\n"," 'lady': tensor(0.1767),\n"," 'lake': tensor(0.0512),\n"," 'lamp': tensor(-0.1435),\n"," 'landed': tensor(0.1606),\n"," 'large': tensor(-0.0338),\n"," 'last': tensor(0.2637),\n"," 'late': tensor(-0.0534),\n"," 'later': tensor(0.1207),\n"," 'laugh': tensor(-0.0152),\n"," 'laughed': tensor(0.0479),\n"," 'laughing': tensor(-0.0570),\n"," 'laughter': tensor(-0.0296),\n"," 'lay': tensor(0.0472),\n"," 'lead': tensor(-0.0302),\n"," 'leading': tensor(0.0861),\n"," 'leaky': tensor(0.0182),\n"," 'leaned': tensor(-0.0899),\n"," 'leapt': tensor(-0.0243),\n"," 'learn': tensor(-0.0530),\n"," 'learned': tensor(-0.2049),\n"," 'least': tensor(0.0334),\n"," 'leave': tensor(0.1629),\n"," 'leaves': tensor(0.0729),\n"," 'leaving': tensor(0.0446),\n"," 'led': tensor(-0.0033),\n"," 'lee': tensor(-0.0277),\n"," 'left': tensor(-0.0180),\n"," 'leg': tensor(-0.0076),\n"," 'legs': tensor(-0.0646),\n"," 'lemon': tensor(0.2030),\n"," 'less': tensor(-0.0702),\n"," 'lesson': tensor(-0.1447),\n"," 'lessons': tensor(0.0786),\n"," 'let': tensor(-0.2263),\n"," 'lets': tensor(-0.0133),\n"," 'letter': tensor(0.0232),\n"," 'letters': tensor(0.2248),\n"," 'library': tensor(-0.0608),\n"," 'lie': tensor(-0.1195),\n"," 'life': tensor(-0.0430),\n"," 'light': tensor(0.0735),\n"," 'lightning': tensor(-0.1557),\n"," 'like': tensor(-0.0403),\n"," 'liked': tensor(-0.1411),\n"," 'lily': tensor(-0.0525),\n"," 'line': tensor(0.1837),\n"," 'lips': tensor(0.2137),\n"," 'list': tensor(0.0340),\n"," 'listen': tensor(-0.0270),\n"," 'listening': tensor(-0.0858),\n"," 'lit': tensor(-0.0802),\n"," 'little': tensor(0.0398),\n"," 'live': tensor(-0.0327),\n"," 'lived': tensor(-0.0983),\n"," 'living': tensor(0.0655),\n"," 'loads': tensor(0.1486),\n"," 'lock': tensor(-0.0526),\n"," 'locked': tensor(-0.1452),\n"," 'london': tensor(-0.1378),\n"," 'long': tensor(0.1372),\n"," 'longbottom': tensor(-0.0919),\n"," 'look': tensor(-0.0299),\n"," 'looked': tensor(-0.1307),\n"," 'looking': tensor(0.0289),\n"," 'looks': tensor(0.0248),\n"," 'lose': tensor(-0.0728),\n"," 'losing': tensor(0.0510),\n"," 'lost': tensor(-0.1759),\n"," 'lot': tensor(0.0083),\n"," 'lots': tensor(-0.0211),\n"," 'loud': tensor(-0.1071),\n"," 'loudly': tensor(0.0927),\n"," 'low': tensor(0.1941),\n"," 'luck': tensor(0.0752),\n"," 'lucky': tensor(0.0538),\n"," 'lumpy': tensor(-0.0510),\n"," 'lurking': tensor(-0.1738),\n"," 'lying': tensor(0.0004),\n"," 'mad': tensor(-0.0110),\n"," 'madam': tensor(0.0043),\n"," 'made': tensor(0.0846),\n"," 'magic': tensor(-0.0300),\n"," 'magical': tensor(0.1794),\n"," 'mail': tensor(0.0684),\n"," 'make': tensor(-0.0994),\n"," 'making': tensor(-0.0981),\n"," 'malfoy': tensor(0.0270),\n"," 'malfoys': tensor(-0.0213),\n"," 'man': tensor(-0.0631),\n"," 'managed': tensor(0.0529),\n"," 'many': tensor(0.2841),\n"," 'marble': tensor(-0.1697),\n"," 'marched': tensor(0.1608),\n"," 'match': tensor(-0.0169),\n"," 'matter': tensor(-0.1528),\n"," 'may': tensor(0.0087),\n"," 'maybe': tensor(-0.1054),\n"," 'mcgonagall': tensor(0.0096),\n"," 'mcgonagalls': tensor(-0.0702),\n"," 'me': tensor(0.1475),\n"," 'mean': tensor(0.0232),\n"," 'means': tensor(-0.1320),\n"," 'meant': tensor(0.1728),\n"," 'meet': tensor(0.0982),\n"," 'mention': tensor(0.0247),\n"," 'met': tensor(    0.0001),\n"," 'midair': tensor(0.1380),\n"," 'middle': tensor(0.0053),\n"," 'midnight': tensor(0.0729),\n"," 'might': tensor(-0.0867),\n"," 'mind': tensor(-0.1663),\n"," 'ministry': tensor(-0.1437),\n"," 'minute': tensor(-0.0696),\n"," 'minutes': tensor(0.1140),\n"," 'mirror': tensor(-0.1047),\n"," 'miss': tensor(0.1628),\n"," 'mistake': tensor(-0.1036),\n"," 'moaned': tensor(0.1065),\n"," 'mom': tensor(0.0289),\n"," 'moment': tensor(0.0772),\n"," 'money': tensor(-0.0511),\n"," 'moonlight': tensor(-0.0286),\n"," 'more': tensor(-0.0298),\n"," 'morning': tensor(-0.1470),\n"," 'most': tensor(-0.0387),\n"," 'mother': tensor(0.1189),\n"," 'mothers': tensor(-0.0978),\n"," 'motorcycle': tensor(0.0105),\n"," 'mountain': tensor(-0.0051),\n"," 'mouth': tensor(0.0256),\n"," 'move': tensor(0.1418),\n"," 'moved': tensor(-0.1753),\n"," 'moving': tensor(0.1988),\n"," 'mr': tensor(0.1535),\n"," 'mrs': tensor(-0.0587),\n"," 'much': tensor(0.0085),\n"," 'muggle': tensor(-0.0344),\n"," 'muggles': tensor(0.0373),\n"," 'murmured': tensor(-0.2406),\n"," 'must': tensor(-0.1043),\n"," 'mustache': tensor(-0.0283),\n"," 'mustve': tensor(-0.0668),\n"," 'muttered': tensor(0.0661),\n"," 'muttering': tensor(0.1799),\n"," 'my': tensor(-0.0783),\n"," 'myself': tensor(0.0784),\n"," 'mysterious': tensor(0.1096),\n"," 'nah': tensor(-0.1086),\n"," 'name': tensor(0.1845),\n"," 'names': tensor(0.0181),\n"," 'narrow': tensor(0.2018),\n"," 'nasty': tensor(0.0987),\n"," 'near': tensor(0.0865),\n"," 'nearer': tensor(-0.0034),\n"," 'nearest': tensor(-0.0175),\n"," 'nearly': tensor(0.0397),\n"," 'neck': tensor(0.0498),\n"," 'need': tensor(-0.0078),\n"," 'needed': tensor(0.1846),\n"," 'needs': tensor(-0.0032),\n"," 'neither': tensor(-0.2368),\n"," 'nervous': tensor(0.1743),\n"," 'nervously': tensor(0.1307),\n"," 'never': tensor(0.0005),\n"," 'neville': tensor(0.0564),\n"," 'nevilles': tensor(-0.0333),\n"," 'new': tensor(-0.1406),\n"," 'news': tensor(-0.1099),\n"," 'newspaper': tensor(0.1531),\n"," 'next': tensor(0.2430),\n"," 'nice': tensor(0.1620),\n"," 'nicolas': tensor(-0.0198),\n"," 'night': tensor(0.1752),\n"," 'nimbus': tensor(0.1343),\n"," 'nine': tensor(-0.0433),\n"," 'no': tensor(0.1349),\n"," 'nobody': tensor(-0.0226),\n"," 'nodded': tensor(0.0297),\n"," 'noise': tensor(0.1061),\n"," 'none': tensor(-0.1137),\n"," 'nor': tensor(0.1090),\n"," 'norbert': tensor(-0.1731),\n"," 'normal': tensor(-0.1104),\n"," 'norris': tensor(0.0468),\n"," 'nose': tensor(0.0100),\n"," 'noses': tensor(0.0891),\n"," 'nostrils': tensor(-0.0923),\n"," 'not': tensor(-0.0549),\n"," 'note': tensor(0.0100),\n"," 'notes': tensor(0.0568),\n"," 'nothin': tensor(-0.0824),\n"," 'nothing': tensor(0.0012),\n"," 'notice': tensor(-0.1499),\n"," 'noticed': tensor(0.1364),\n"," 'noticing': tensor(-0.0659),\n"," 'now': tensor(0.0578),\n"," 'number': tensor(-0.0175),\n"," 'o': tensor(0.1008),\n"," 'obviously': tensor(-0.0003),\n"," 'oclock': tensor(-0.0328),\n"," 'odd': tensor(0.1595),\n"," 'of': tensor(-0.2273),\n"," 'off': tensor(0.1607),\n"," 'often': tensor(0.0880),\n"," 'oh': tensor(-0.1758),\n"," 'old': tensor(0.0043),\n"," 'older': tensor(-0.0636),\n"," 'ollivander': tensor(0.1760),\n"," 'on': tensor(0.2299),\n"," 'once': tensor(-0.0242),\n"," 'one': tensor(-0.1274),\n"," 'ones': tensor(0.0023),\n"," 'only': tensor(-0.0150),\n"," 'onto': tensor(0.0284),\n"," 'open': tensor(0.0378),\n"," 'opened': tensor(0.0147),\n"," 'opposite': tensor(0.0971),\n"," 'or': tensor(0.1496),\n"," 'ordinary': tensor(0.1850),\n"," 'other': tensor(0.0218),\n"," 'others': tensor(-0.1695),\n"," 'our': tensor(0.1827),\n"," 'out': tensor(0.0436),\n"," 'outside': tensor(-0.1827),\n"," 'outta': tensor(-0.3056),\n"," 'over': tensor(-0.1783),\n"," 'overhead': tensor(-0.1906),\n"," 'owl': tensor(-0.2440),\n"," 'owls': tensor(0.0054),\n"," 'own': tensor(-0.0364),\n"," 'pack': tensor(0.0653),\n"," 'package': tensor(-0.1342),\n"," 'packed': tensor(-0.1535),\n"," 'pain': tensor(0.1760),\n"," 'pair': tensor(0.0973),\n"," 'pale': tensor(-0.0986),\n"," 'panted': tensor(-0.0141),\n"," 'paper': tensor(0.0510),\n"," 'parcel': tensor(-0.1012),\n"," 'parchment': tensor(0.0744),\n"," 'parents': tensor(0.0327),\n"," 'particularly': tensor(-0.1420),\n"," 'passageway': tensor(-0.1299),\n"," 'passed': tensor(-0.0544),\n"," 'passing': tensor(-0.2553),\n"," 'past': tensor(0.0478),\n"," 'path': tensor(0.0435),\n"," 'patil': tensor(-0.1225),\n"," 'peered': tensor(-0.0713),\n"," 'peering': tensor(0.0610),\n"," 'peeves': tensor(-0.0752),\n"," 'people': tensor(0.0192),\n"," 'percy': tensor(-0.2265),\n"," 'perfect': tensor(0.0604),\n"," 'perhaps': tensor(0.1034),\n"," 'person': tensor(0.0746),\n"," 'petunia': tensor(0.0411),\n"," 'picked': tensor(-0.0752),\n"," 'piece': tensor(0.0258),\n"," 'pieces': tensor(-0.0138),\n"," 'piers': tensor(-0.0692),\n"," 'pig': tensor(-0.1052),\n"," 'pile': tensor(0.0030),\n"," 'piled': tensor(0.1162),\n"," 'pink': tensor(-0.0699),\n"," 'pinned': tensor(-0.0170),\n"," 'place': tensor(0.0208),\n"," 'planets': tensor(-0.0141),\n"," 'plant': tensor(-0.0722),\n"," 'plates': tensor(0.2251),\n"," 'platform': tensor(0.2077),\n"," 'platforms': tensor(0.2017),\n"," 'play': tensor(0.0843),\n"," 'players': tensor(0.0606),\n"," 'playing': tensor(0.1473),\n"," 'please': tensor(-0.0671),\n"," 'pleased': tensor(-0.0910),\n"," 'pocket': tensor(0.0522),\n"," 'pockets': tensor(0.0123),\n"," 'point': tensor(-0.0664),\n"," 'pointed': tensor(0.0191),\n"," 'pointing': tensor(-0.0501),\n"," 'points': tensor(-0.0353),\n"," 'pomfrey': tensor(0.1731),\n"," 'poor': tensor(-0.0251),\n"," 'portrait': tensor(0.1702),\n"," 'possession': tensor(0.2064),\n"," 'possible': tensor(0.0560),\n"," 'posts': tensor(0.1254),\n"," 'potion': tensor(-0.0196),\n"," 'potions': tensor(-0.0009),\n"," 'potter': tensor(-0.1254),\n"," 'potters': tensor(-0.0617),\n"," 'power': tensor(-0.0121),\n"," 'powerful': tensor(-0.0033),\n"," 'practice': tensor(-0.0504),\n"," 'prefect': tensor(-0.1060),\n"," 'prefects': tensor(0.0342),\n"," 'presents': tensor(-0.0870),\n"," 'pressed': tensor(0.1799),\n"," 'privet': tensor(0.0694),\n"," 'probably': tensor(0.0201),\n"," 'professor': tensor(-0.1000),\n"," 'properly': tensor(-0.0519),\n"," 'proud': tensor(-0.0438),\n"," 'pull': tensor(-0.0619),\n"," 'pulled': tensor(0.0843),\n"," 'pulling': tensor(-0.3022),\n"," 'purple': tensor(0.1284),\n"," 'pushed': tensor(-0.0761),\n"," 'put': tensor(0.1312),\n"," 'quaffle': tensor(-0.2615),\n"," 'question': tensor(0.1053),\n"," 'questions': tensor(-0.0415),\n"," 'quick': tensor(-0.1239),\n"," 'quickly': tensor(0.0176),\n"," 'quidditch': tensor(0.1490),\n"," 'quiet': tensor(0.0747),\n"," 'quietly': tensor(-0.1156),\n"," 'quills': tensor(0.1776),\n"," 'quirrell': tensor(-0.1775),\n"," 'quirrells': tensor(-0.0371),\n"," 'quite': tensor(0.1520),\n"," 'racing': tensor(0.1710),\n"," 'raised': tensor(0.0633),\n"," 'ran': tensor(-0.0895),\n"," 'rang': tensor(-0.0667),\n"," 'rat': tensor(0.0845),\n"," 'rather': tensor(0.1113),\n"," 'ravenclaw': tensor(0.0819),\n"," 'reached': tensor(0.1942),\n"," 'read': tensor(0.2282),\n"," 'ready': tensor(-0.0028),\n"," 'real': tensor(0.0466),\n"," 'realize': tensor(0.3324),\n"," 'realized': tensor(0.0559),\n"," 'really': tensor(0.0262),\n"," 'reason': tensor(0.1070),\n"," ...}"]},"metadata":{},"execution_count":58}],"source":["# we can get the dot product value for every other words in the vocab\n","# to get  P(word | harry)\n","word_dot_dict = {}\n","for word in filtered_vocab:\n","  w_idx = word2idx[word]\n","  w_vector = word_vectors[w_idx]\n","  word_dot_dict[word] = sum(harry * w_vector)\n","word_dot_dict"]},{"cell_type":"markdown","metadata":{"id":"MQzYZK-gkzqe"},"source":["Now, let's convert these dot products to probabilities using the softmax function:\n","- We have to convert our prediction into probability distribution to get P(word|harry) so that sum of [P(a|harry), ..., P(potter|harry), ... P(ron|harry), ... ] = 1\n","- current dot product value is any real number, sometimes called as logit\n","  - logit from logistic regression. Some values that are not yet converted to 0-1 or value before sigmoid function\n","  - every probability should be in range (0, 1) (greater than 0, smaller than 1)\n","  - this can be handled by taking exponential of dot product values, divided by total sum\n","  - This function is called **Softmax**\n","\n","- Why we use exponential?\n","  - Because we want to make every probability in positive range while preserving the order\n"]},{"cell_type":"code","execution_count":59,"metadata":{"id":"DQ1PUvuLyv6r","executionInfo":{"status":"ok","timestamp":1742276747866,"user_tz":-540,"elapsed":2,"user":{"displayName":"SunMyeong Lee","userId":"02273362555347354442"}}},"outputs":[],"source":["from math import exp\n","word_exp_dict = {}\n","for word, dot_value in word_dot_dict.items():\n","  word_exp_dict[word] = exp(dot_value)\n","word_exp_dict\n","\n","word_prob_dict = None"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SpXH7RxSbwUO"},"outputs":[],"source":["# Get P(potter|harry)"]},{"cell_type":"markdown","metadata":{"id":"kKxz9SJhSuhy"},"source":["## 13. Efficient Matrix Operations\n","![img](https://mkang32.github.io/images/python/khan_academy_matrix_product.png)\n","\n","Instead of calculating dot products one by one, we can use matrix multiplication for efficiency:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HCFajiDkb44W"},"outputs":[],"source":["# get dot product result for every word in the vocabulary\n","\n","# first, make vector_of_harry into matrix format\n","\n","# do matrix multiplication\n"]},{"cell_type":"markdown","metadata":{"id":"03qx3olEkzqe"},"source":["Let's verify that our matrix multiplication gives the same result as individual dot products:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wGpJCtJQkzqe"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"x2tW_Q6kkzqe"},"source":["Now let's implement the complete softmax calculation using matrix operations:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5USdps5CfKzq"},"outputs":[],"source":["# convert dot product result into exponential"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WNq0gqz0fntB"},"outputs":[],"source":["# get the sum of exponential\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ruV4_myDgLvG"},"outputs":[],"source":["# divide exponential value with sum"]},{"cell_type":"markdown","metadata":{"id":"QJ228pigkzqf"},"source":["## 14. Creating a Probability Function\n","\n","Let's create a function to calculate probabilities efficiently:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EF4gutuEgntj"},"outputs":[],"source":["def get_probs(query_vectors, entire_vectors):\n","  return None\n","\n","# get_probs(mat_of_harry, word_vectors)"]},{"cell_type":"markdown","metadata":{"id":"HyVcT_6wkzqf"},"source":["## 15. Preparing for Training\n","\n","Before training our Word2Vec model, we need to split our dataset into training and testing sets:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3gjCSY3DoaFg"},"outputs":[],"source":["# Now we can train the word2vec\n","\n","# Let's think about training pairs\n","index_pairs # this is our dataset. It's list of list of two integer\n","# two integer means a pair of neighboring words\n","\n","# Training set and Test set\n","# To validate that our model can solve 'unseen' problems\n","# So we have to split the dataset before training.\n","\n","# To randomly split the dataset, we will first shuffle the dataset\n","\n","# random.shuffle(index_pairs) # this will shuffle the list items"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TC0QF3NZp9Rh"},"outputs":[],"source":["len(train_set), len(test_set)"]},{"cell_type":"markdown","metadata":{"id":"P7m_ZB-Vkzqf"},"source":["## 16. Training the Word2Vec Model\n","\n","Now we'll train our Word2Vec model using batched gradient descent:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JpUM3aiiqG8e"},"outputs":[],"source":["# making batch from train_set\n","# Batch is a set of training samples, that are calculated together\n","# And also we update the model after one single batch"]},{"cell_type":"markdown","metadata":{"id":"fuUAzGoukzqf"},"source":["## 17. Evaluating the Training\n","\n","Let's visualize the training loss to see if our model is learning:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FJHSV8zbyLYu"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","plt.plot(loss_record)"]},{"cell_type":"markdown","metadata":{"id":"7agT-5NPkzqf"},"source":["## 18. Testing the Model\n","\n","Now we'll test our model on the test set:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RDP9aR48zdJu"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"F2ZOErd0kzqf"},"source":["## 19. Exploring Learned Word Relationships\n","\n","Let's explore what our model has learned by finding the words most closely related to \"harry\":"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0YaCDNq_0hFc"},"outputs":[],"source":["# P(potter|harry)?\n"]}],"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/jdasam/aat3020/blob/2025/notebooks/1_word2vec.ipynb","timestamp":1741669229458}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":0}