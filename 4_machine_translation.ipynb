{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sunmyeonglee/2025-1-NLP/blob/main/4_machine_translation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qY1O7pxaV97t"
      },
      "source": [
        "# Live Coding: Machine Translation (Korean to English) with Seq2Seq\n",
        "\n",
        "Welcome! In this session, we'll build a neural machine translation (NMT) system to translate Korean sentences into English. We'll use PyTorch and concepts from sequence-to-sequence (Seq2Seq) modeling."
      ],
      "id": "qY1O7pxaV97t"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLyT0DVoV97u"
      },
      "source": [
        "## 1. Setting up the Environment ğŸ› ï¸\n",
        "\n",
        "First, we need to install the libraries that we'll be using:\n",
        "-   `transformers`: From Hugging Face, for easy access to tokenizers and potentially pre-trained model components (though we'll build our own tokenizers and Seq2Seq model).\n",
        "-   `tokenizers`: Hugging Face's library for training our own fast tokenizers.\n",
        "-   `gdown`: To download datasets/files from Google Drive.\n",
        "-   `pandas`: For data manipulation, especially to load our dataset.\n",
        "\n",
        "Let's install them!"
      ],
      "id": "aLyT0DVoV97u"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FEQM4hNRV97u"
      },
      "outputs": [],
      "source": [
        "# Install necessary libraries\n",
        "!pip install transformers tokenizers pandas gdown --quiet"
      ],
      "id": "FEQM4hNRV97u"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjTUsbmNV97v"
      },
      "source": [
        "## 2. Downloading the Dataset ğŸ“š\n",
        "\n",
        "Machine translation models require a **parallel corpus**, which is a collection of texts in a source language aligned with their translations in a target language.\n",
        "\n",
        "We'll use a Korean-English parallel corpus (originally from NIA AI-Hub). The command below will download a CSV version of this dataset."
      ],
      "id": "TjTUsbmNV97v"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w4AIl_3xV97v",
        "outputId": "dd4b32dc-fa79-4e07-d1ac-330a06a7663c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset downloaded and unzipped.\n"
          ]
        }
      ],
      "source": [
        "# Download the dataset\n",
        "!gdown 13CGLEULYccogSLByHXPAxSveLZTtnj8c --quiet\n",
        "!unzip -q -o nia_korean_english_csv.zip # -o overwrites if exists\n",
        "\n",
        "print(\"Dataset downloaded and unzipped.\")"
      ],
      "id": "w4AIl_3xV97v"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tU9I54TfV97v"
      },
      "source": [
        "## 3. Loading and Inspecting the Data ğŸ§\n",
        "\n",
        "Now that our dataset is downloaded, let's load it using `pandas` and take a first look. We expect to see Korean sentences and their corresponding English translations."
      ],
      "id": "tU9I54TfV97v"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GADp7hREV97w",
        "outputId": "7850a236-f505-4fe3-8613-c681b33adf01"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First 5 rows of the dataset:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ì›ë¬¸</th>\n",
              "      <th>ë²ˆì—­ë¬¸</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>'Bible Coloring'ì€ ì„±ê²½ì˜ ì•„ë¦„ë‹¤ìš´ ì´ì•¼ê¸°ë¥¼ ì²´í—˜ í•  ìˆ˜ ìˆëŠ” ì»¬ëŸ¬ë§ ...</td>\n",
              "      <td>Bible Coloring' is a coloring application that...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ì”¨í‹°ì€í–‰ì—ì„œ ì¼í•˜ì„¸ìš”?</td>\n",
              "      <td>Do you work at a City bank?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>í‘¸ë¦¬í† ì˜ ë² ìŠ¤íŠ¸ì…€ëŸ¬ëŠ” í•´ì™¸ì—ì„œ ì…ì†Œë¬¸ë§Œìœ¼ë¡œ 4ì°¨ ì™„íŒì„ ê¸°ë¡í•˜ì˜€ë‹¤.</td>\n",
              "      <td>PURITO's bestseller, which recorded 4th rough ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11ì¥ì—ì„œëŠ” ì˜ˆìˆ˜ë‹˜ì´ ì´ë²ˆì—” ë‚˜ì‚¬ë¡œë¥¼ ë¬´ë¤ì—ì„œ ë¶ˆëŸ¬ë‚´ì–´ ì£½ì€ ì ê°€ìš´ë°ì„œ ì‚´ë¦¬ì…¨ìŠµë‹ˆë‹¤.</td>\n",
              "      <td>In Chapter 11 Jesus called Lazarus from the to...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6.5, 7, 8 ì‚¬ì´ì¦ˆê°€ ëª‡ ê°œë‚˜ ë” ì¬ì…ê³  ë ì§€ ì œê²Œ ì•Œë ¤ì£¼ì‹œë©´ ê°ì‚¬í•˜ê² ìŠµë‹ˆë‹¤.</td>\n",
              "      <td>I would feel grateful to know how many stocks ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  ì›ë¬¸  \\\n",
              "0  'Bible Coloring'ì€ ì„±ê²½ì˜ ì•„ë¦„ë‹¤ìš´ ì´ì•¼ê¸°ë¥¼ ì²´í—˜ í•  ìˆ˜ ìˆëŠ” ì»¬ëŸ¬ë§ ...   \n",
              "1                                       ì”¨í‹°ì€í–‰ì—ì„œ ì¼í•˜ì„¸ìš”?   \n",
              "2              í‘¸ë¦¬í† ì˜ ë² ìŠ¤íŠ¸ì…€ëŸ¬ëŠ” í•´ì™¸ì—ì„œ ì…ì†Œë¬¸ë§Œìœ¼ë¡œ 4ì°¨ ì™„íŒì„ ê¸°ë¡í•˜ì˜€ë‹¤.   \n",
              "3   11ì¥ì—ì„œëŠ” ì˜ˆìˆ˜ë‹˜ì´ ì´ë²ˆì—” ë‚˜ì‚¬ë¡œë¥¼ ë¬´ë¤ì—ì„œ ë¶ˆëŸ¬ë‚´ì–´ ì£½ì€ ì ê°€ìš´ë°ì„œ ì‚´ë¦¬ì…¨ìŠµë‹ˆë‹¤.   \n",
              "4     6.5, 7, 8 ì‚¬ì´ì¦ˆê°€ ëª‡ ê°œë‚˜ ë” ì¬ì…ê³  ë ì§€ ì œê²Œ ì•Œë ¤ì£¼ì‹œë©´ ê°ì‚¬í•˜ê² ìŠµë‹ˆë‹¤.   \n",
              "\n",
              "                                                 ë²ˆì—­ë¬¸  \n",
              "0  Bible Coloring' is a coloring application that...  \n",
              "1                        Do you work at a City bank?  \n",
              "2  PURITO's bestseller, which recorded 4th rough ...  \n",
              "3  In Chapter 11 Jesus called Lazarus from the to...  \n",
              "4  I would feel grateful to know how many stocks ...  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "DataFrame Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1602418 entries, 0 to 1602417\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count    Dtype \n",
            "---  ------  --------------    ----- \n",
            " 0   ì›ë¬¸      1602418 non-null  object\n",
            " 1   ë²ˆì—­ë¬¸     1602418 non-null  object\n",
            "dtypes: object(2)\n",
            "memory usage: 24.5+ MB\n",
            "\n",
            "Sample sentence pairs:\n",
            "  Korean (ì›ë¬¸): 'Bible Coloring'ì€ ì„±ê²½ì˜ ì•„ë¦„ë‹¤ìš´ ì´ì•¼ê¸°ë¥¼ ì²´í—˜ í•  ìˆ˜ ìˆëŠ” ì»¬ëŸ¬ë§ ì•±ì…ë‹ˆë‹¤.\n",
            "  English (ë²ˆì—­ë¬¸): Bible Coloring' is a coloring application that allows you to experience beautiful stories in the Bible.\n",
            "--------------------\n",
            "  Korean (ì›ë¬¸): ì”¨í‹°ì€í–‰ì—ì„œ ì¼í•˜ì„¸ìš”?\n",
            "  English (ë²ˆì—­ë¬¸): Do you work at a City bank?\n",
            "--------------------\n",
            "  Korean (ì›ë¬¸): í‘¸ë¦¬í† ì˜ ë² ìŠ¤íŠ¸ì…€ëŸ¬ëŠ” í•´ì™¸ì—ì„œ ì…ì†Œë¬¸ë§Œìœ¼ë¡œ 4ì°¨ ì™„íŒì„ ê¸°ë¡í•˜ì˜€ë‹¤.\n",
            "  English (ë²ˆì—­ë¬¸): PURITO's bestseller, which recorded 4th rough -cuts by words of mouth from abroad.\n",
            "--------------------\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "# Define the path to the CSV file\n",
        "csv_file_path = \"nia_korean_english.csv\" # This should match the unzipped file name\n",
        "\n",
        "# Load the dataframe\n",
        "df = pd.read_csv(csv_file_path)\n",
        "\n",
        "# Display the first few rows\n",
        "print(\"First 5 rows of the dataset:\")\n",
        "display(df.head())\n",
        "\n",
        "# Display some info about the dataframe\n",
        "print(\"\\nDataFrame Info:\")\n",
        "df.info()\n",
        "\n",
        "# Display a few examples\n",
        "print(\"\\nSample sentence pairs:\")\n",
        "for i in range(3):\n",
        "    print(f\"  Korean (ì›ë¬¸): {df['ì›ë¬¸'].iloc[i]}\")\n",
        "    print(f\"  English (ë²ˆì—­ë¬¸): {df['ë²ˆì—­ë¬¸'].iloc[i]}\")\n",
        "    print(\"-\" * 20)"
      ],
      "id": "GADp7hREV97w"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XylwcQvEV97w"
      },
      "source": [
        "## 4. Tokenization: Turning Text into Numbers ğŸ”¢\n",
        "\n",
        "Neural networks don't understand words directly. They need numerical input. **Tokenization** is the process of converting text into a sequence of numerical IDs. This involves:\n",
        "1.  **Splitting** text into smaller units called **tokens** (words, sub-words, or characters).\n",
        "2.  Building a **vocabulary**: a mapping from unique tokens to integer IDs.\n",
        "3.  **Converting** sequences of tokens into sequences of IDs.\n",
        "\n",
        "We'll use the `BertWordPieceTokenizer` from the `tokenizers` library. WordPiece is effective because it can break down unknown words into known sub-word units. We need to train two separate tokenizers: one for Korean (source) and one for English (target).\n",
        "\n",
        "### 4.1. Preparing Data for Tokenizer Training\n",
        "\n",
        "The tokenizer trainer expects input as a list of paths to text files. Let's extract our Korean and English sentences into separate `.txt` files."
      ],
      "id": "XylwcQvEV97w"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T2RFwEmFV97w",
        "outputId": "ec289334-75d2-477f-a313-1fcbe3a2ed5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Korean corpus saved to: tokenizer_data/korean_corpus.txt\n",
            "English corpus saved to: tokenizer_data/english_corpus.txt\n"
          ]
        }
      ],
      "source": [
        "# Create a directory for tokenizer data\n",
        "tokenizer_data_dir = Path(\"tokenizer_data\")\n",
        "tokenizer_data_dir.mkdir(exist_ok=True)\n",
        "\n",
        "# Define file paths for the corpus\n",
        "korean_corpus_file = tokenizer_data_dir / \"korean_corpus.txt\"\n",
        "english_corpus_file = tokenizer_data_dir / \"english_corpus.txt\"\n",
        "\n",
        "# Save Korean sentences (make sure they are strings)\n",
        "with open(korean_corpus_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    for sentence in df['ì›ë¬¸']:\n",
        "        f.write(str(sentence) + \"\\n\")\n",
        "\n",
        "# Save English sentences (make sure they are strings)\n",
        "with open(english_corpus_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    for sentence in df['ë²ˆì—­ë¬¸']:\n",
        "        f.write(str(sentence) + \"\\n\")\n",
        "\n",
        "print(f\"Korean corpus saved to: {korean_corpus_file}\")\n",
        "print(f\"English corpus saved to: {english_corpus_file}\")"
      ],
      "id": "T2RFwEmFV97w"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qi8PlwfoV97w"
      },
      "source": [
        "### 4.2. Training the Tokenizers\n",
        "\n",
        "Now, let's train the `BertWordPieceTokenizer` for Korean and English.\n",
        "Key parameters:\n",
        "-   `vocab_size`: The maximum number of unique tokens the tokenizer will learn.\n",
        "-   `min_frequency`: A token must appear at least this many times to be included in the vocabulary.\n",
        "-   `limit_alphabet`: Limits the number of initial characters considered to build the vocabulary.\n",
        "-   `special_tokens`: We define standard BERT special tokens like `[PAD]` (padding), `[UNK]` (unknown), `[CLS]` (classification/start), `[SEP]` (separator/end)."
      ],
      "id": "Qi8PlwfoV97w"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KLvTBtzvV97x",
        "outputId": "f55d0031-c6f8-4a31-982f-6198dafdd793"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Korean tokenizer...\n",
            "\n",
            "\n",
            "\n",
            "Korean tokenizer saved to: hugging_kor_32000\n",
            "\n",
            "Training English tokenizer...\n",
            "\n",
            "\n",
            "\n",
            "English tokenizer saved to: hugging_eng_32000\n"
          ]
        }
      ],
      "source": [
        "from tokenizers import BertWordPieceTokenizer\n",
        "\n",
        "# Tokenizer parameters\n",
        "VOCAB_SIZE = 32000\n",
        "MIN_FREQUENCY = 5\n",
        "LIMIT_ALPHABET = 6000 # How many initial characters to look at\n",
        "SPECIAL_TOKENS = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
        "\n",
        "# --- Train Korean Tokenizer ---\n",
        "korean_tokenizer_output_dir = Path(f'hugging_kor_{VOCAB_SIZE}')\n",
        "korean_tokenizer_output_dir.mkdir(exist_ok=True)\n",
        "\n",
        "kor_tokenizer_trainer = BertWordPieceTokenizer(\n",
        "    strip_accents=False, # Keep accents\n",
        "    lowercase=False      # Preserve case\n",
        ")\n",
        "\n",
        "print(\"Training Korean tokenizer...\")\n",
        "kor_tokenizer_trainer.train(\n",
        "    files=[str(korean_corpus_file)],\n",
        "    vocab_size=VOCAB_SIZE,\n",
        "    min_frequency=MIN_FREQUENCY,\n",
        "    limit_alphabet=LIMIT_ALPHABET,\n",
        "    show_progress=True,\n",
        "    special_tokens=SPECIAL_TOKENS\n",
        ")\n",
        "kor_tokenizer_trainer.save_model(str(korean_tokenizer_output_dir))\n",
        "print(f\"Korean tokenizer saved to: {korean_tokenizer_output_dir}\")\n",
        "\n",
        "\n",
        "# --- Train English Tokenizer ---\n",
        "english_tokenizer_output_dir = Path(f'hugging_eng_{VOCAB_SIZE}')\n",
        "english_tokenizer_output_dir.mkdir(exist_ok=True)\n",
        "\n",
        "eng_tokenizer_trainer = BertWordPieceTokenizer(\n",
        "    strip_accents=False, # Usually True for English, but False is fine\n",
        "    lowercase=False      # Usually True for English, but False helps if case is important\n",
        ")\n",
        "\n",
        "print(\"\\nTraining English tokenizer...\")\n",
        "eng_tokenizer_trainer.train(\n",
        "    files=[str(english_corpus_file)],\n",
        "    vocab_size=VOCAB_SIZE,\n",
        "    min_frequency=MIN_FREQUENCY,\n",
        "    limit_alphabet=LIMIT_ALPHABET,\n",
        "    show_progress=True,\n",
        "    special_tokens=SPECIAL_TOKENS\n",
        ")\n",
        "eng_tokenizer_trainer.save_model(str(english_tokenizer_output_dir))\n",
        "print(f\"English tokenizer saved to: {english_tokenizer_output_dir}\")"
      ],
      "id": "KLvTBtzvV97x"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaz7pV7xV97x"
      },
      "source": [
        "### 4.3. Loading and Testing Trained Tokenizers\n",
        "\n",
        "We can now load our trained tokenizers using `BertTokenizerFast` from the `transformers` library. This provides a convenient interface for encoding text to IDs and decoding IDs back to text."
      ],
      "id": "uaz7pV7xV97x"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_eCbZ9hhV97x",
        "outputId": "e9b5009f-fa7d-4f8e-9e7c-6eaad6a71feb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/teo/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Korean: ì´ê²ƒì€ í•œêµ­ì–´ í† í¬ë‚˜ì´ì € í…ŒìŠ¤íŠ¸ì…ë‹ˆë‹¤.\n",
            "Tokens: ['ì´ê²ƒì€', 'í•œêµ­ì–´', 'í† í¬', '##ë‚˜ì´', '##ì €', 'í…ŒìŠ¤íŠ¸', '##ì…ë‹ˆë‹¤', '.']\n",
            "Token IDs: [2, 8062, 8698, 16135, 15425, 4311, 10222, 6461, 18, 3]\n",
            "Decoded: [CLS] ì´ê²ƒì€ í•œêµ­ì–´ í† í¬ë‚˜ì´ì € í…ŒìŠ¤íŠ¸ì…ë‹ˆë‹¤. [SEP]\n",
            "\n",
            "Original English: This is an English tokenizer test.\n",
            "Tokens: ['this', 'is', 'an', 'eng', '##lish', 'token', '##izer', 'test', '.']\n",
            "Token IDs: [2, 1200, 1056, 1112, 3058, 2566, 15803, 10469, 2356, 18, 3]\n",
            "Decoded: [CLS] this is an english tokenizer test. [SEP]\n",
            "\n",
            "Source (Korean) tokenizer vocab size: 32000\n",
            "Target (English) tokenizer vocab size: 32000\n",
            "Source PAD ID: 0, CLS ID: 2, SEP ID: 3\n",
            "Target PAD ID: 0, CLS ID: 2, SEP ID: 3\n"
          ]
        }
      ],
      "source": [
        "from transformers import BertTokenizerFast\n",
        "\n",
        "# Load the trained Korean (source) tokenizer\n",
        "tokenizer_src = BertTokenizerFast.from_pretrained(\n",
        "    str(korean_tokenizer_output_dir),\n",
        "    strip_accents=False,\n",
        "    lowercase=False\n",
        ")\n",
        "\n",
        "# Load the trained English (target) tokenizer\n",
        "tokenizer_tgt = BertTokenizerFast.from_pretrained(\n",
        "    str(english_tokenizer_output_dir),\n",
        "    strip_accents=False,\n",
        "    lowercase=False\n",
        ")\n",
        "\n",
        "# Test the Korean tokenizer\n",
        "sample_korean_sentence = \"ì´ê²ƒì€ í•œêµ­ì–´ í† í¬ë‚˜ì´ì € í…ŒìŠ¤íŠ¸ì…ë‹ˆë‹¤.\"\n",
        "print(f\"Original Korean: {sample_korean_sentence}\")\n",
        "tokenized_src_sample = tokenizer_src(sample_korean_sentence)\n",
        "print(f\"Tokens: {tokenizer_src.tokenize(sample_korean_sentence)}\")\n",
        "print(f\"Token IDs: {tokenized_src_sample['input_ids']}\")\n",
        "print(f\"Decoded: {tokenizer_src.decode(tokenized_src_sample['input_ids'])}\")\n",
        "\n",
        "# Test the English tokenizer\n",
        "sample_english_sentence = \"This is an English tokenizer test.\"\n",
        "print(f\"\\nOriginal English: {sample_english_sentence}\")\n",
        "tokenized_tgt_sample = tokenizer_tgt(sample_english_sentence)\n",
        "print(f\"Tokens: {tokenizer_tgt.tokenize(sample_english_sentence)}\")\n",
        "print(f\"Token IDs: {tokenized_tgt_sample['input_ids']}\")\n",
        "print(f\"Decoded: {tokenizer_tgt.decode(tokenized_tgt_sample['input_ids'])}\")\n",
        "\n",
        "# Vocabulary sizes\n",
        "print(f\"\\nSource (Korean) tokenizer vocab size: {tokenizer_src.vocab_size}\")\n",
        "print(f\"Target (English) tokenizer vocab size: {tokenizer_tgt.vocab_size}\")\n",
        "print(f\"Source PAD ID: {tokenizer_src.pad_token_id}, CLS ID: {tokenizer_src.cls_token_id}, SEP ID: {tokenizer_src.sep_token_id}\")\n",
        "print(f\"Target PAD ID: {tokenizer_tgt.pad_token_id}, CLS ID: {tokenizer_tgt.cls_token_id}, SEP ID: {tokenizer_tgt.sep_token_id}\")"
      ],
      "id": "_eCbZ9hhV97x"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHI1WkIFV97x"
      },
      "source": [
        "## 5. Creating a PyTorch Dataset ğŸ“¦\n",
        "\n",
        "PyTorch's `Dataset` class provides an abstraction over our data. We'll create a custom `TranslationDataset` that will:\n",
        "1.  Take a DataFrame (train, val, or test) and our tokenizers.\n",
        "2.  In its `__getitem__` method, fetch a Korean-English pair.\n",
        "3.  Tokenize them.\n",
        "4.  Return the tokenized source sentence, tokenized target sentence (for decoder input), and labels (also from target sentence, for loss calculation).\n",
        "\n",
        "For a Seq2Seq model, typical inputs/outputs per sample are:\n",
        "-   `encoder_input_ids`: Tokenized source sentence.\n",
        "-   `decoder_input_ids`: Tokenized target sentence, usually starting with a start-of-sequence (SOS) token (e.g., `[CLS]`). This is fed to the decoder during training (teacher forcing).\n",
        "-   `labels`: Tokenized target sentence, usually ending with an end-of-sequence (EOS) token (e.g., `[SEP]`). This is what the decoder aims to predict. Often, labels are a shifted version of `decoder_input_ids`.\n",
        "\n",
        "Our `BertTokenizerFast` automatically adds `[CLS]` (start) and `[SEP]` (end) tokens."
      ],
      "id": "QHI1WkIFV97x"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UmJN5BY6V97x",
        "outputId": "c95e5ea7-07c4-4150-8365-e286728b9fd2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample from TranslationDataset:\n",
            "  Source IDs: tensor([    2,    11,    70,  4360,  4551, 13306,    71, 12901,  9564, 12435,\n",
            "           11,  3546, 14567,  4326,  8934,  8407,  7400,  4154,  3252,  6420,\n",
            "        12985,  5025,  3397,  6461,    18,     3])\n",
            "  Source Decoded: [CLS]'bible coloring'ì€ ì„±ê²½ì˜ ì•„ë¦„ë‹¤ìš´ ì´ì•¼ê¸°ë¥¼ ì²´í—˜ í•  ìˆ˜ ìˆëŠ” ì»¬ëŸ¬ë§ ì•±ì…ë‹ˆë‹¤. [SEP]\n",
            "  Target (Decoder Input) IDs: tensor([    2, 26268, 23067,    11,  1056,    69, 23067,  2803,  1067,  5155,\n",
            "         1117,  1042,  2405,  4024,  5520,  1039,  1023, 26268,    18])\n",
            "  Target (Decoder Input) Decoded: [CLS] bible coloring'is a coloring application that allows you to experience beautiful stories in the bible.\n",
            "  Labels IDs: tensor([26268, 23067,    11,  1056,    69, 23067,  2803,  1067,  5155,  1117,\n",
            "         1042,  2405,  4024,  5520,  1039,  1023, 26268,    18,     3])\n",
            "  Labels Decoded: bible coloring'is a coloring application that allows you to experience beautiful stories in the bible. [SEP]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, dataframe, src_tokenizer, tgt_tokenizer, src_col=\"ì›ë¬¸\", tgt_col=\"ë²ˆì—­ë¬¸\", max_length=128):\n",
        "        self.dataframe = dataframe\n",
        "        self.src_tokenizer = src_tokenizer\n",
        "        self.tgt_tokenizer = tgt_tokenizer\n",
        "        self.src_col = src_col\n",
        "        self.tgt_col = tgt_col\n",
        "        self.max_length = max_length # Max sequence length for truncation\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src_text = str(self.dataframe.iloc[idx][self.src_col])\n",
        "        tgt_text = str(self.dataframe.iloc[idx][self.tgt_col])\n",
        "\n",
        "        # Tokenize source sentence\n",
        "        # `encode` returns a list of IDs. We convert to tensor.\n",
        "        # `add_special_tokens=True` adds [CLS] and [SEP]\n",
        "        encoder_input_ids = torch.tensor(\n",
        "            self.src_tokenizer.encode(src_text, add_special_tokens=True, truncation=True, max_length=self.max_length)\n",
        "        )\n",
        "\n",
        "        # Tokenize target sentence for both decoder input and labels\n",
        "        # For many Transformer-based Seq2Seq, decoder_input and labels can be the same tokenized target sequence.\n",
        "        # The causal attention mask in the decoder ensures it only attends to previous positions.\n",
        "        # For RNNs, decoder_input is often <SOS> + target_tokens and labels are target_tokens + <EOS>.\n",
        "        # Since our tokenizers add [CLS] and [SEP], we can use the tokenized sequence directly.\n",
        "        target_token_ids = torch.tensor(\n",
        "            self.tgt_tokenizer.encode(tgt_text, add_special_tokens=True, truncation=True, max_length=self.max_length)\n",
        "        )\n",
        "\n",
        "        # In the original notebook, pack_collate expects (source, target, shifted_target)\n",
        "        # Let's prepare them such that:\n",
        "        # - source = encoder_input_ids\n",
        "        # - target = decoder_input_ids (e.g. [CLS] w1 w2)\n",
        "        # - shifted_target = labels (e.g. w1 w2 [SEP])\n",
        "        decoder_input_ids = target_token_ids[:-1]\n",
        "        labels = target_token_ids[1:]\n",
        "\n",
        "        return encoder_input_ids, decoder_input_ids, labels\n",
        "\n",
        "# Create Dataset instances\n",
        "MAX_SEQ_LENGTH = 100 # Define a max length for sequences\n",
        "dataset = TranslationDataset(df, tokenizer_src, tokenizer_tgt, max_length=MAX_SEQ_LENGTH)\n",
        "# Test a sample from the dataset\n",
        "sample_src_ids, sample_tgt_ids, sample_lbl_ids = dataset[0]\n",
        "print(\"Sample from TranslationDataset:\")\n",
        "print(f\"  Source IDs: {sample_src_ids}\")\n",
        "print(f\"  Source Decoded: {tokenizer_src.decode(sample_src_ids)}\")\n",
        "print(f\"  Target (Decoder Input) IDs: {sample_tgt_ids}\")\n",
        "print(f\"  Target (Decoder Input) Decoded: {tokenizer_tgt.decode(sample_tgt_ids)}\")\n",
        "print(f\"  Labels IDs: {sample_lbl_ids}\")\n",
        "print(f\"  Labels Decoded: {tokenizer_tgt.decode(sample_lbl_ids)}\")"
      ],
      "id": "UmJN5BY6V97x"
    },
    {
      "cell_type": "markdown",
      "id": "2f86fbd3",
      "metadata": {
        "id": "2f86fbd3"
      },
      "source": [
        "## 6. Splitting the Dataset\n",
        "\n",
        "To train and evaluate our model robustly, we split our data into three sets:\n",
        "1.  **Training set**: Used to train the model parameters.\n",
        "2.  **Validation set**: Used during training to monitor performance, tune hyperparameters, and prevent overfitting.\n",
        "3.  **Test set**: Used *only once* at the very end to get an unbiased evaluation of the final model.\n",
        "\n",
        "We'll use a common split (e.g., 80% train, 10% validation, 10% test). For faster execution in this live session, we might use a subset of the full data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bab4d36",
      "metadata": {
        "id": "2bab4d36",
        "outputId": "6adbf373-2595-4f41-e941-7c37676baf8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train dataset size: 1281935\n",
            "Validation dataset size: 160242\n",
            "Test dataset size: 160241\n"
          ]
        }
      ],
      "source": [
        "train_ratio = 0.8\n",
        "val_ratio = 0.1\n",
        "test_ratio = 0.1\n",
        "\n",
        "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_ratio, val_ratio, 1.0 - train_ratio - val_ratio], generator=torch.Generator().manual_seed(42))\n",
        "\n",
        "print(f\"Train dataset size: {len(train_dataset)}\")\n",
        "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
        "print(f\"Test dataset size: {len(test_dataset)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELiaX4MCV97y"
      },
      "source": [
        "## 7. DataLoader and Collate Function for Variable Length Sequences ğŸ”„\n",
        "\n",
        "Sentences have different lengths. When creating batches of data with `DataLoader`, we need to handle these varying lengths.\n",
        "-   **Padding**: Making all sequences in a batch the same length by adding `[PAD]` tokens.\n",
        "-   **Packing**: More efficient for RNNs. `torch.nn.utils.rnn.pack_sequence` sorts sequences by length, concatenates non-padded elements, and stores `batch_sizes` (how many sequences are active at each timestep). PyTorch RNNs can process `PackedSequence` efficiently.\n",
        "\n",
        "We'll define a **collate function**. This function is passed to `DataLoader` and takes a list of samples (from our `Dataset`) to form a batch. Our collate function will use `pack_sequence`."
      ],
      "id": "ELiaX4MCV97y"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kaOySvA-V97y"
      },
      "outputs": [],
      "source": [
        "from torch.nn.utils.rnn import pack_sequence, PackedSequence\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def pack_collate_fn(raw_batch_list):\n",
        "    # raw_batch_list is a list of tuples: [(src1, tgt_in1, lbl1), (src2, tgt_in2, lbl2), ...]\n",
        "    # Each src_i, tgt_in_i, lbl_i is a 1D tensor of token IDs.\n",
        "\n",
        "    sources, target_inputs, labels = zip(*raw_batch_list)\n",
        "    # Now, sources = (src1, src2, ...), target_inputs = (tgt_in1, tgt_in2, ...), etc.\n",
        "\n",
        "    # `pack_sequence` expects a list of Tensors. It will sort them by length (descending)\n",
        "    # if enforce_sorted=False (which is the default).\n",
        "    packed_sources = pack_sequence(sources, enforce_sorted=False)\n",
        "    packed_target_inputs = pack_sequence(target_inputs, enforce_sorted=False)\n",
        "    packed_labels = pack_sequence(labels, enforce_sorted=False)\n",
        "\n",
        "    return packed_sources, packed_target_inputs, packed_labels\n",
        "\n",
        "# Create DataLoaders\n",
        "BATCH_SIZE = 64 # Adjust based on GPU memory\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, collate_fn=pack_collate_fn, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, collate_fn=pack_collate_fn, shuffle=False, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, collate_fn=pack_collate_fn, shuffle=False, num_workers=2)\n",
        "\n",
        "# Let's test the DataLoader and collate function\n",
        "print(\"Testing DataLoader and pack_collate_fn...\")\n",
        "try:\n",
        "    batch_src_packed, batch_tgt_in_packed, batch_lbl_packed = next(iter(train_loader))\n",
        "    print(\"Batch loaded successfully!\")\n",
        "\n",
        "    print(\"\\nSource Batch (PackedSequence):\")\n",
        "    print(f\"  Data shape (flattened tokens): {batch_src_packed.data.shape}\")\n",
        "    print(f\"  Batch sizes (active sequences per timestep): {batch_src_packed.batch_sizes}\")\n",
        "    # print(f\"  Sorted indices (original pos of sorted seqs): {batch_src_packed.sorted_indices}\")\n",
        "    # print(f\"  Unsorted indices (how to restore original order): {batch_src_packed.unsorted_indices}\")\n",
        "\n",
        "    print(\"\\nTarget Input Batch (PackedSequence):\")\n",
        "    print(f\"  Data shape: {batch_tgt_in_packed.data.shape}\")\n",
        "    print(f\"  Batch sizes: {batch_tgt_in_packed.batch_sizes}\")\n",
        "\n",
        "    print(\"\\nLabels Batch (PackedSequence):\")\n",
        "    print(f\"  Data shape: {batch_lbl_packed.data.shape}\")\n",
        "    print(f\"  Batch sizes: {batch_lbl_packed.batch_sizes}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error loading batch: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ],
      "id": "kaOySvA-V97y"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zP9QBszV97y"
      },
      "source": [
        "## 8. Defining the Sequence-to-Sequence (Seq2Seq) Model ğŸ§ \n",
        "\n",
        "We'll build a classic Encoder-Decoder model using GRU (Gated Recurrent Unit) layers.\n",
        "\n",
        "![Seq2Seq Architecture](https://raw.githubusercontent.com/tensorflow/nmt/master/nmt/g3doc/img/seq2seq.jpg)\n",
        "*(Image source: TensorFlow NMT Tutorial)*\n",
        "\n",
        "**Encoder**:\n",
        "1.  **Embedding Layer**: Converts input source tokens (Korean) into dense vectors. We'll use `padding_idx` so PAD tokens have a zero embedding and don't contribute to gradients.\n",
        "2.  **GRU Layer**: Processes the sequence of embeddings and outputs all hidden states and the final hidden state (context vector).\n",
        "\n",
        "**Decoder**:\n",
        "1.  **Embedding Layer**: Converts input target tokens (English) into dense vectors.\n",
        "2.  **GRU Layer**: Takes the current target token's embedding and the previous hidden state (initialized with the encoder's context vector) to generate an output.\n",
        "3.  **Linear Layer (Projection)**: Maps the GRU's output to logits over the target vocabulary.\n",
        "\n",
        "Our model components will handle `PackedSequence` inputs. The encoder's final hidden state must be correctly passed to the decoder, potentially reordering it if the source and target sequences within a batch were sorted differently by `pack_sequence`."
      ],
      "id": "4zP9QBszV97y"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f4mI2FguV97y"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import PackedSequence # Already imported but good for clarity\n",
        "\n",
        "import torch.nn as nn\n",
        "class Seq2seq(nn.Module):\n",
        "  def __init__(self, enc_vocab, dec_vocab, hidden_size, num_layers=2):\n",
        "    super().__init__()\n",
        "    self.encoder = Encoder(enc_vocab, hidden_size, num_layers=num_layers)\n",
        "    self.decoder = Decoder(dec_vocab, hidden_size, num_layers=num_layers)\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self, num_vocab, hidden_size, num_layers=2):\n",
        "    super().__init__()\n",
        "    self.emb = nn.Embedding(num_vocab, hidden_size)\n",
        "    self.rnn = nn.GRU(hidden_size, hidden_size, batch_first=True, num_layers=num_layers)\n",
        "    # batch_first True: it takes (Num_samples_in_batch, num_timesteps, num_dim)\n",
        "    # batch_first False: it takes (num_timesteps, Num_samples_in_batch, num_dim)\n",
        "\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self, num_vocab, hidden_size, num_layers=2):\n",
        "    super().__init__()\n",
        "    self.emb = nn.Embedding(num_vocab, hidden_size)\n",
        "    self.rnn = nn.GRU(hidden_size, hidden_size, batch_first=True, num_layers=num_layers)\n",
        "    self.proj = nn.Linear(hidden_size, num_vocab)\n",
        "\n"
      ],
      "id": "f4mI2FguV97y"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1395db77",
      "metadata": {
        "id": "1395db77"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c32b22e",
      "metadata": {
        "id": "5c32b22e"
      },
      "outputs": [],
      "source": [
        "\n",
        "# --- Model Hyperparameters & Instantiation ---\n",
        "EMBEDDING_DIM = 256\n",
        "HIDDEN_DIM = 512  # Hidden dimension for GRU\n",
        "NUM_LAYERS = 2    # Number of GRU layers\n",
        "DROPOUT_P = 0.3   # Dropout probability\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Instantiate model components\n",
        "enc = Encoder(tokenizer_src.vocab_size, EMBEDDING_DIM, HIDDEN_DIM, NUM_LAYERS, DROPOUT_P, tokenizer_src.pad_token_id).to(device)\n",
        "dec = Decoder(tokenizer_tgt.vocab_size, EMBEDDING_DIM, HIDDEN_DIM, NUM_LAYERS, DROPOUT_P, tokenizer_tgt.pad_token_id).to(device)\n",
        "model = Seq2Seq(enc, dec, device).to(device)\n",
        "\n",
        "print(f\"Model created with {sum(p.numel() for p in model.parameters() if p.requires_grad):,} trainable parameters.\")\n",
        "\n",
        "# Test forward pass with a batch (if batch variables exist from DataLoader test)\n",
        "if 'batch_src_packed' in locals() and 'batch_tgt_in_packed' in locals():\n",
        "    print(\"\\nTesting model forward pass with a sample batch...\")\n",
        "    try:\n",
        "        # Ensure batch tensors are on the correct device\n",
        "        src_dev = PackedSequence(batch_src_packed.data.to(device), batch_src_packed.batch_sizes, batch_src_packed.sorted_indices, batch_src_packed.unsorted_indices)\n",
        "        tgt_in_dev = PackedSequence(batch_tgt_in_packed.data.to(device), batch_tgt_in_packed.batch_sizes, batch_tgt_in_packed.sorted_indices, batch_tgt_in_packed.unsorted_indices)\n",
        "\n",
        "        model.train() # Set to train mode for consistent behavior if dropout/batchnorm were used differently\n",
        "        output_logits_packed = model(src_dev, tgt_in_dev)\n",
        "\n",
        "        print(\"Model forward pass successful!\")\n",
        "        print(f\"Output (PackedSequence logits data shape): {output_logits_packed.data.shape}\")\n",
        "        # Expected: (total_num_target_tokens_in_batch, target_vocab_size)\n",
        "    except Exception as e:\n",
        "        print(f\"Error during model forward pass test: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "else:\n",
        "    print(\"\\nSkipping model forward pass test as batch data is not loaded. Run DataLoader cell first.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afnQnU-jV97y"
      },
      "source": [
        "## 9. Training the Model ğŸ”¥\n",
        "\n",
        "The training process involves iterating through the training dataset in epochs and batches:\n",
        "1.  **Get batch**: From `train_loader`.\n",
        "2.  **Zero gradients**: `optimizer.zero_grad()`.\n",
        "3.  **Forward pass**: `model(source_batch, target_input_batch)` to get `output_logits`.\n",
        "4.  **Calculate loss**: Compare `output_logits.data` with `labels_batch.data`. `CrossEntropyLoss` is suitable, and its `ignore_index` parameter should be set to the PAD token ID for the target language so padding doesn't contribute to the loss.\n",
        "5.  **Backward pass**: `loss.backward()` to compute gradients.\n",
        "6.  **Gradient clipping**: (Optional but recommended) `torch.nn.utils.clip_grad_norm_` to prevent exploding gradients.\n",
        "7.  **Optimizer step**: `optimizer.step()` to update model weights.\n",
        "\n",
        "We'll also define an evaluation function to check performance on the validation set.\n",
        "\n",
        "**Note**: Full training takes time. For this live session, we'll define the loop structure and then load pre-trained weights."
      ],
      "id": "afnQnU-jV97y"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-1qR0UWrV97y"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "import time\n",
        "import math\n",
        "\n",
        "# Define Loss Function\n",
        "# The output of our model (PackedSequence.data) is [total_target_tokens, target_vocab_size]\n",
        "# The labels (PackedSequence.data) are [total_target_tokens]\n",
        "# This is exactly what CrossEntropyLoss expects.\n",
        "TARGET_PAD_IDX = tokenizer_tgt.pad_token_id\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=TARGET_PAD_IDX)\n",
        "\n",
        "# Define Optimizer\n",
        "LEARNING_RATE = 1e-3 # 0.001\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# Training configuration\n",
        "N_EPOCHS = 1 # Set higher for actual training (e.g., 10-20)\n",
        "CLIP = 1.0     # Gradient clipping value\n",
        "\n",
        "def train_epoch(model, dataloader, optimizer, criterion, clip, device):\n",
        "    model.train() # Set model to training mode\n",
        "    epoch_loss = 0\n",
        "    num_batches = len(dataloader)\n",
        "\n",
        "    for i, (src_packed, tgt_in_packed, lbl_packed) in enumerate(dataloader):\n",
        "        # Move batch data to the device\n",
        "        src_dev = PackedSequence(src_packed.data.to(device), src_packed.batch_sizes, src_packed.sorted_indices, src_packed.unsorted_indices)\n",
        "        tgt_in_dev = PackedSequence(tgt_in_packed.data.to(device), tgt_in_packed.batch_sizes, tgt_in_packed.sorted_indices, tgt_in_packed.unsorted_indices)\n",
        "        lbl_dev_data = lbl_packed.data.to(device) # Only need .data for labels\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass: output_logits_packed.data is (sum_lengths, vocab_size)\n",
        "        output_logits_packed = model(src_dev, tgt_in_dev)\n",
        "\n",
        "        # Calculate loss: output_logits_packed.data vs lbl_dev_data\n",
        "        # output_logits_packed.data shape: (total_tokens_in_batch, target_vocab_size)\n",
        "        # lbl_dev_data shape: (total_tokens_in_batch)\n",
        "        loss = criterion(output_logits_packed.data, lbl_dev_data)\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip) # Clip gradients\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        if (i + 1) % (num_batches // 10 if num_batches >= 10 else 1) == 0: # Print progress ~10 times\n",
        "            print(f'  Batch {i+1}/{num_batches} | Train Loss: {loss.item():.3f}')\n",
        "\n",
        "    return epoch_loss / num_batches\n",
        "\n",
        "def evaluate_epoch(model, dataloader, criterion, device):\n",
        "    model.eval() # Set model to evaluation mode\n",
        "    epoch_loss = 0\n",
        "    with torch.no_grad(): # Disable gradient calculations\n",
        "        for i, (src_packed, tgt_in_packed, lbl_packed) in enumerate(dataloader):\n",
        "            src_dev = PackedSequence(src_packed.data.to(device), src_packed.batch_sizes, src_packed.sorted_indices, src_packed.unsorted_indices)\n",
        "            tgt_in_dev = PackedSequence(tgt_in_packed.data.to(device), tgt_in_packed.batch_sizes, tgt_in_packed.sorted_indices, tgt_in_packed.unsorted_indices)\n",
        "            lbl_dev_data = lbl_packed.data.to(device)\n",
        "\n",
        "            output_logits_packed = model(src_dev, tgt_in_dev)\n",
        "            loss = criterion(output_logits_packed.data, lbl_dev_data)\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(dataloader)\n",
        "\n",
        "print(\"Training and evaluation loop structures defined.\")\n",
        "print(\"We will skip actual training and load pre-trained weights for the live session.\")\n",
        "\n",
        "# --- Example of starting a training loop (commented out for live coding) ---\n",
        "# best_valid_loss = float('inf')\n",
        "# MODEL_SAVE_PATH = 'best-seq2seq-model.pt'\n",
        "\n",
        "# for epoch in range(N_EPOCHS):\n",
        "#     start_time = time.time()\n",
        "#     print(f'Starting Epoch: {epoch+1:02}/{N_EPOCHS}')\n",
        "\n",
        "#     train_loss = train_epoch(model, train_loader, optimizer, criterion, CLIP, device)\n",
        "#     valid_loss = evaluate_epoch(model, val_loader, criterion, device)\n",
        "\n",
        "#     end_time = time.time()\n",
        "#     epoch_mins, epoch_secs = divmod(end_time - start_time, 60)\n",
        "\n",
        "#     if valid_loss < best_valid_loss:\n",
        "#         best_valid_loss = valid_loss\n",
        "#         torch.save({\n",
        "#             'epoch': epoch,\n",
        "#             'model_state_dict': model.state_dict(),\n",
        "#             'optimizer_state_dict': optimizer.state_dict(),\n",
        "#             'loss': best_valid_loss,\n",
        "#             }, MODEL_SAVE_PATH)\n",
        "#         print(f'  ** Best validation loss: {best_valid_loss:.3f}. Model saved to {MODEL_SAVE_PATH} **')\n",
        "\n",
        "#     print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {int(epoch_secs)}s')\n",
        "#     print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "#     print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ],
      "id": "-1qR0UWrV97y"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g54Zwb68V97y"
      },
      "source": [
        "## 10. Loading Pre-trained Weights ğŸ’¾\n",
        "\n",
        "Training a good NMT model from scratch can take many hours or days. To save time, we'll load pre-trained weights into our model structure.\n",
        "\n",
        "**Important**: The architecture of the model we define (embedding dimensions, hidden sizes, number of layers, vocabulary sizes) *must exactly match* the architecture used when these weights were saved.\n",
        "\n",
        "The original notebook used `hidden_size = 512` and `num_layers=3` for its pre-trained weights. Let's define a new model instance with these parameters to load the weights."
      ],
      "id": "g54Zwb68V97y"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5L2pWh6-V97y"
      },
      "outputs": [],
      "source": [
        "# Download the pre-trained model weights\n",
        "!gdown 15jL2TaRk6Q47uuPWDruUge6O_gCPv5mp --quiet -O kor_eng_translator_model_vanilla_best.pt\n",
        "print(\"Pre-trained weights 'kor_eng_translator_model_vanilla_best.pt' downloaded.\")\n",
        "\n",
        "# Parameters matching the pre-trained model (from original notebook's loading cell)\n",
        "PRETRAINED_EMBEDDING_DIM = EMBEDDING_DIM # Assuming embedding dim was consistent, adjust if needed\n",
        "PRETRAINED_HIDDEN_DIM = 512\n",
        "PRETRAINED_NUM_LAYERS = 3\n",
        "PRETRAINED_DROPOUT_P = DROPOUT_P # Assuming dropout was consistent\n",
        "\n",
        "print(f\"\\nInstantiating model for pre-trained weights:\")\n",
        "print(f\"  Embedding Dim: {PRETRAINED_EMBEDDING_DIM}\")\n",
        "print(f\"  Hidden Dim: {PRETRAINED_HIDDEN_DIM}\")\n",
        "print(f\"  Num Layers: {PRETRAINED_NUM_LAYERS}\")\n",
        "print(f\"  Dropout: {PRETRAINED_DROPOUT_P}\")\n",
        "print(f\"  Source Vocab Size: {tokenizer_src.vocab_size}\")\n",
        "print(f\"  Target Vocab Size: {tokenizer_tgt.vocab_size}\")\n",
        "\n",
        "\n",
        "# Instantiate the model with parameters matching the pre-trained file\n",
        "enc_loaded = Encoder(tokenizer_src.vocab_size, PRETRAINED_EMBEDDING_DIM, PRETRAINED_HIDDEN_DIM,\n",
        "                     PRETRAINED_NUM_LAYERS, PRETRAINED_DROPOUT_P, tokenizer_src.pad_token_id).to(device)\n",
        "dec_loaded = Decoder(tokenizer_tgt.vocab_size, PRETRAINED_EMBEDDING_DIM, PRETRAINED_HIDDEN_DIM,\n",
        "                     PRETRAINED_NUM_LAYERS, PRETRAINED_DROPOUT_P, tokenizer_tgt.pad_token_id).to(device)\n",
        "model_loaded = Seq2Seq(enc_loaded, dec_loaded, device).to(device)\n",
        "\n",
        "try:\n",
        "    # The checkpoint file contains a dictionary, and the model's state_dict is under the 'model' key.\n",
        "    checkpoint = torch.load(\"kor_eng_translator_model_vanilla_best.pt\", map_location=device)\n",
        "    model_loaded.load_state_dict(checkpoint['model'])\n",
        "    print(\"\\nPre-trained model weights loaded successfully!\")\n",
        "    print(f\"Loaded model has {sum(p.numel() for p in model_loaded.parameters()):,} total parameters.\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nError loading pre-trained weights: {e}\")\n",
        "    print(\"Ensure the model architecture (vocab sizes, dimensions, layers) matches the saved state_dict.\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ],
      "id": "5L2pWh6-V97y"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYTQe5dnV97z"
      },
      "source": [
        "## 11. Translation (Inference) ğŸ—£ï¸â¡ï¸ğŸ’¬\n",
        "\n",
        "With our (pre-trained) model ready, let's translate some new Korean sentences into English!\n",
        "The inference process (also called decoding) works step-by-step:\n",
        "1.  Tokenize the input Korean sentence.\n",
        "2.  Pass tokenized input through the **Encoder** to get the context vector (final hidden state).\n",
        "3.  Initialize the **Decoder** with this context vector.\n",
        "4.  Start decoding with a special start-of-sequence (SOS) token (e.g., `[CLS]`).\n",
        "5.  In a loop, for each step:\n",
        "    a.  Feed the current generated token (or SOS for the first step) and the previous decoder hidden state to the decoder.\n",
        "    b.  Get the logits (raw scores) over the target vocabulary.\n",
        "    c.  Select the token with the highest score (this is called **greedy decoding**).\n",
        "    d.  If the selected token is an end-of-sequence (EOS) token (e.g., `[SEP]`), stop.\n",
        "    e.  Otherwise, add the token to our list of output tokens and use it as input for the next decoding step.\n",
        "6.  Convert the list of output token IDs back into an English sentence."
      ],
      "id": "PYTQe5dnV97z"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QNYlZYC8V97z"
      },
      "outputs": [],
      "source": [],
      "id": "QNYlZYC8V97z"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}