{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sunmyeonglee/2025-1-NLP/blob/main/4_machine_translation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qY1O7pxaV97t"
      },
      "source": [
        "# Live Coding: Machine Translation (Korean to English) with Seq2Seq\n",
        "\n",
        "Welcome! In this session, we'll build a neural machine translation (NMT) system to translate Korean sentences into English. We'll use PyTorch and concepts from sequence-to-sequence (Seq2Seq) modeling."
      ],
      "id": "qY1O7pxaV97t"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLyT0DVoV97u"
      },
      "source": [
        "## 1. Setting up the Environment 🛠️\n",
        "\n",
        "First, we need to install the libraries that we'll be using:\n",
        "-   `transformers`: From Hugging Face, for easy access to tokenizers and potentially pre-trained model components (though we'll build our own tokenizers and Seq2Seq model).\n",
        "-   `tokenizers`: Hugging Face's library for training our own fast tokenizers.\n",
        "-   `gdown`: To download datasets/files from Google Drive.\n",
        "-   `pandas`: For data manipulation, especially to load our dataset.\n",
        "\n",
        "Let's install them!"
      ],
      "id": "aLyT0DVoV97u"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FEQM4hNRV97u"
      },
      "outputs": [],
      "source": [
        "# Install necessary libraries\n",
        "!pip install transformers tokenizers pandas gdown --quiet"
      ],
      "id": "FEQM4hNRV97u"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjTUsbmNV97v"
      },
      "source": [
        "## 2. Downloading the Dataset 📚\n",
        "\n",
        "Machine translation models require a **parallel corpus**, which is a collection of texts in a source language aligned with their translations in a target language.\n",
        "\n",
        "We'll use a Korean-English parallel corpus (originally from NIA AI-Hub). The command below will download a CSV version of this dataset."
      ],
      "id": "TjTUsbmNV97v"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w4AIl_3xV97v",
        "outputId": "dd4b32dc-fa79-4e07-d1ac-330a06a7663c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset downloaded and unzipped.\n"
          ]
        }
      ],
      "source": [
        "# Download the dataset\n",
        "!gdown 13CGLEULYccogSLByHXPAxSveLZTtnj8c --quiet\n",
        "!unzip -q -o nia_korean_english_csv.zip # -o overwrites if exists\n",
        "\n",
        "print(\"Dataset downloaded and unzipped.\")"
      ],
      "id": "w4AIl_3xV97v"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tU9I54TfV97v"
      },
      "source": [
        "## 3. Loading and Inspecting the Data 🧐\n",
        "\n",
        "Now that our dataset is downloaded, let's load it using `pandas` and take a first look. We expect to see Korean sentences and their corresponding English translations."
      ],
      "id": "tU9I54TfV97v"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GADp7hREV97w",
        "outputId": "7850a236-f505-4fe3-8613-c681b33adf01"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First 5 rows of the dataset:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>원문</th>\n",
              "      <th>번역문</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>'Bible Coloring'은 성경의 아름다운 이야기를 체험 할 수 있는 컬러링 ...</td>\n",
              "      <td>Bible Coloring' is a coloring application that...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>씨티은행에서 일하세요?</td>\n",
              "      <td>Do you work at a City bank?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>푸리토의 베스트셀러는 해외에서 입소문만으로 4차 완판을 기록하였다.</td>\n",
              "      <td>PURITO's bestseller, which recorded 4th rough ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11장에서는 예수님이 이번엔 나사로를 무덤에서 불러내어 죽은 자 가운데서 살리셨습니다.</td>\n",
              "      <td>In Chapter 11 Jesus called Lazarus from the to...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6.5, 7, 8 사이즈가 몇 개나 더 재입고 될지 제게 알려주시면 감사하겠습니다.</td>\n",
              "      <td>I would feel grateful to know how many stocks ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  원문  \\\n",
              "0  'Bible Coloring'은 성경의 아름다운 이야기를 체험 할 수 있는 컬러링 ...   \n",
              "1                                       씨티은행에서 일하세요?   \n",
              "2              푸리토의 베스트셀러는 해외에서 입소문만으로 4차 완판을 기록하였다.   \n",
              "3   11장에서는 예수님이 이번엔 나사로를 무덤에서 불러내어 죽은 자 가운데서 살리셨습니다.   \n",
              "4     6.5, 7, 8 사이즈가 몇 개나 더 재입고 될지 제게 알려주시면 감사하겠습니다.   \n",
              "\n",
              "                                                 번역문  \n",
              "0  Bible Coloring' is a coloring application that...  \n",
              "1                        Do you work at a City bank?  \n",
              "2  PURITO's bestseller, which recorded 4th rough ...  \n",
              "3  In Chapter 11 Jesus called Lazarus from the to...  \n",
              "4  I would feel grateful to know how many stocks ...  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "DataFrame Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1602418 entries, 0 to 1602417\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count    Dtype \n",
            "---  ------  --------------    ----- \n",
            " 0   원문      1602418 non-null  object\n",
            " 1   번역문     1602418 non-null  object\n",
            "dtypes: object(2)\n",
            "memory usage: 24.5+ MB\n",
            "\n",
            "Sample sentence pairs:\n",
            "  Korean (원문): 'Bible Coloring'은 성경의 아름다운 이야기를 체험 할 수 있는 컬러링 앱입니다.\n",
            "  English (번역문): Bible Coloring' is a coloring application that allows you to experience beautiful stories in the Bible.\n",
            "--------------------\n",
            "  Korean (원문): 씨티은행에서 일하세요?\n",
            "  English (번역문): Do you work at a City bank?\n",
            "--------------------\n",
            "  Korean (원문): 푸리토의 베스트셀러는 해외에서 입소문만으로 4차 완판을 기록하였다.\n",
            "  English (번역문): PURITO's bestseller, which recorded 4th rough -cuts by words of mouth from abroad.\n",
            "--------------------\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "# Define the path to the CSV file\n",
        "csv_file_path = \"nia_korean_english.csv\" # This should match the unzipped file name\n",
        "\n",
        "# Load the dataframe\n",
        "df = pd.read_csv(csv_file_path)\n",
        "\n",
        "# Display the first few rows\n",
        "print(\"First 5 rows of the dataset:\")\n",
        "display(df.head())\n",
        "\n",
        "# Display some info about the dataframe\n",
        "print(\"\\nDataFrame Info:\")\n",
        "df.info()\n",
        "\n",
        "# Display a few examples\n",
        "print(\"\\nSample sentence pairs:\")\n",
        "for i in range(3):\n",
        "    print(f\"  Korean (원문): {df['원문'].iloc[i]}\")\n",
        "    print(f\"  English (번역문): {df['번역문'].iloc[i]}\")\n",
        "    print(\"-\" * 20)"
      ],
      "id": "GADp7hREV97w"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XylwcQvEV97w"
      },
      "source": [
        "## 4. Tokenization: Turning Text into Numbers 🔢\n",
        "\n",
        "Neural networks don't understand words directly. They need numerical input. **Tokenization** is the process of converting text into a sequence of numerical IDs. This involves:\n",
        "1.  **Splitting** text into smaller units called **tokens** (words, sub-words, or characters).\n",
        "2.  Building a **vocabulary**: a mapping from unique tokens to integer IDs.\n",
        "3.  **Converting** sequences of tokens into sequences of IDs.\n",
        "\n",
        "We'll use the `BertWordPieceTokenizer` from the `tokenizers` library. WordPiece is effective because it can break down unknown words into known sub-word units. We need to train two separate tokenizers: one for Korean (source) and one for English (target).\n",
        "\n",
        "### 4.1. Preparing Data for Tokenizer Training\n",
        "\n",
        "The tokenizer trainer expects input as a list of paths to text files. Let's extract our Korean and English sentences into separate `.txt` files."
      ],
      "id": "XylwcQvEV97w"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T2RFwEmFV97w",
        "outputId": "ec289334-75d2-477f-a313-1fcbe3a2ed5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Korean corpus saved to: tokenizer_data/korean_corpus.txt\n",
            "English corpus saved to: tokenizer_data/english_corpus.txt\n"
          ]
        }
      ],
      "source": [
        "# Create a directory for tokenizer data\n",
        "tokenizer_data_dir = Path(\"tokenizer_data\")\n",
        "tokenizer_data_dir.mkdir(exist_ok=True)\n",
        "\n",
        "# Define file paths for the corpus\n",
        "korean_corpus_file = tokenizer_data_dir / \"korean_corpus.txt\"\n",
        "english_corpus_file = tokenizer_data_dir / \"english_corpus.txt\"\n",
        "\n",
        "# Save Korean sentences (make sure they are strings)\n",
        "with open(korean_corpus_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    for sentence in df['원문']:\n",
        "        f.write(str(sentence) + \"\\n\")\n",
        "\n",
        "# Save English sentences (make sure they are strings)\n",
        "with open(english_corpus_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    for sentence in df['번역문']:\n",
        "        f.write(str(sentence) + \"\\n\")\n",
        "\n",
        "print(f\"Korean corpus saved to: {korean_corpus_file}\")\n",
        "print(f\"English corpus saved to: {english_corpus_file}\")"
      ],
      "id": "T2RFwEmFV97w"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qi8PlwfoV97w"
      },
      "source": [
        "### 4.2. Training the Tokenizers\n",
        "\n",
        "Now, let's train the `BertWordPieceTokenizer` for Korean and English.\n",
        "Key parameters:\n",
        "-   `vocab_size`: The maximum number of unique tokens the tokenizer will learn.\n",
        "-   `min_frequency`: A token must appear at least this many times to be included in the vocabulary.\n",
        "-   `limit_alphabet`: Limits the number of initial characters considered to build the vocabulary.\n",
        "-   `special_tokens`: We define standard BERT special tokens like `[PAD]` (padding), `[UNK]` (unknown), `[CLS]` (classification/start), `[SEP]` (separator/end)."
      ],
      "id": "Qi8PlwfoV97w"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KLvTBtzvV97x",
        "outputId": "f55d0031-c6f8-4a31-982f-6198dafdd793"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Korean tokenizer...\n",
            "\n",
            "\n",
            "\n",
            "Korean tokenizer saved to: hugging_kor_32000\n",
            "\n",
            "Training English tokenizer...\n",
            "\n",
            "\n",
            "\n",
            "English tokenizer saved to: hugging_eng_32000\n"
          ]
        }
      ],
      "source": [
        "from tokenizers import BertWordPieceTokenizer\n",
        "\n",
        "# Tokenizer parameters\n",
        "VOCAB_SIZE = 32000\n",
        "MIN_FREQUENCY = 5\n",
        "LIMIT_ALPHABET = 6000 # How many initial characters to look at\n",
        "SPECIAL_TOKENS = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
        "\n",
        "# --- Train Korean Tokenizer ---\n",
        "korean_tokenizer_output_dir = Path(f'hugging_kor_{VOCAB_SIZE}')\n",
        "korean_tokenizer_output_dir.mkdir(exist_ok=True)\n",
        "\n",
        "kor_tokenizer_trainer = BertWordPieceTokenizer(\n",
        "    strip_accents=False, # Keep accents\n",
        "    lowercase=False      # Preserve case\n",
        ")\n",
        "\n",
        "print(\"Training Korean tokenizer...\")\n",
        "kor_tokenizer_trainer.train(\n",
        "    files=[str(korean_corpus_file)],\n",
        "    vocab_size=VOCAB_SIZE,\n",
        "    min_frequency=MIN_FREQUENCY,\n",
        "    limit_alphabet=LIMIT_ALPHABET,\n",
        "    show_progress=True,\n",
        "    special_tokens=SPECIAL_TOKENS\n",
        ")\n",
        "kor_tokenizer_trainer.save_model(str(korean_tokenizer_output_dir))\n",
        "print(f\"Korean tokenizer saved to: {korean_tokenizer_output_dir}\")\n",
        "\n",
        "\n",
        "# --- Train English Tokenizer ---\n",
        "english_tokenizer_output_dir = Path(f'hugging_eng_{VOCAB_SIZE}')\n",
        "english_tokenizer_output_dir.mkdir(exist_ok=True)\n",
        "\n",
        "eng_tokenizer_trainer = BertWordPieceTokenizer(\n",
        "    strip_accents=False, # Usually True for English, but False is fine\n",
        "    lowercase=False      # Usually True for English, but False helps if case is important\n",
        ")\n",
        "\n",
        "print(\"\\nTraining English tokenizer...\")\n",
        "eng_tokenizer_trainer.train(\n",
        "    files=[str(english_corpus_file)],\n",
        "    vocab_size=VOCAB_SIZE,\n",
        "    min_frequency=MIN_FREQUENCY,\n",
        "    limit_alphabet=LIMIT_ALPHABET,\n",
        "    show_progress=True,\n",
        "    special_tokens=SPECIAL_TOKENS\n",
        ")\n",
        "eng_tokenizer_trainer.save_model(str(english_tokenizer_output_dir))\n",
        "print(f\"English tokenizer saved to: {english_tokenizer_output_dir}\")"
      ],
      "id": "KLvTBtzvV97x"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaz7pV7xV97x"
      },
      "source": [
        "### 4.3. Loading and Testing Trained Tokenizers\n",
        "\n",
        "We can now load our trained tokenizers using `BertTokenizerFast` from the `transformers` library. This provides a convenient interface for encoding text to IDs and decoding IDs back to text."
      ],
      "id": "uaz7pV7xV97x"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_eCbZ9hhV97x",
        "outputId": "e9b5009f-fa7d-4f8e-9e7c-6eaad6a71feb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/teo/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Korean: 이것은 한국어 토크나이저 테스트입니다.\n",
            "Tokens: ['이것은', '한국어', '토크', '##나이', '##저', '테스트', '##입니다', '.']\n",
            "Token IDs: [2, 8062, 8698, 16135, 15425, 4311, 10222, 6461, 18, 3]\n",
            "Decoded: [CLS] 이것은 한국어 토크나이저 테스트입니다. [SEP]\n",
            "\n",
            "Original English: This is an English tokenizer test.\n",
            "Tokens: ['this', 'is', 'an', 'eng', '##lish', 'token', '##izer', 'test', '.']\n",
            "Token IDs: [2, 1200, 1056, 1112, 3058, 2566, 15803, 10469, 2356, 18, 3]\n",
            "Decoded: [CLS] this is an english tokenizer test. [SEP]\n",
            "\n",
            "Source (Korean) tokenizer vocab size: 32000\n",
            "Target (English) tokenizer vocab size: 32000\n",
            "Source PAD ID: 0, CLS ID: 2, SEP ID: 3\n",
            "Target PAD ID: 0, CLS ID: 2, SEP ID: 3\n"
          ]
        }
      ],
      "source": [
        "from transformers import BertTokenizerFast\n",
        "\n",
        "# Load the trained Korean (source) tokenizer\n",
        "tokenizer_src = BertTokenizerFast.from_pretrained(\n",
        "    str(korean_tokenizer_output_dir),\n",
        "    strip_accents=False,\n",
        "    lowercase=False\n",
        ")\n",
        "\n",
        "# Load the trained English (target) tokenizer\n",
        "tokenizer_tgt = BertTokenizerFast.from_pretrained(\n",
        "    str(english_tokenizer_output_dir),\n",
        "    strip_accents=False,\n",
        "    lowercase=False\n",
        ")\n",
        "\n",
        "# Test the Korean tokenizer\n",
        "sample_korean_sentence = \"이것은 한국어 토크나이저 테스트입니다.\"\n",
        "print(f\"Original Korean: {sample_korean_sentence}\")\n",
        "tokenized_src_sample = tokenizer_src(sample_korean_sentence)\n",
        "print(f\"Tokens: {tokenizer_src.tokenize(sample_korean_sentence)}\")\n",
        "print(f\"Token IDs: {tokenized_src_sample['input_ids']}\")\n",
        "print(f\"Decoded: {tokenizer_src.decode(tokenized_src_sample['input_ids'])}\")\n",
        "\n",
        "# Test the English tokenizer\n",
        "sample_english_sentence = \"This is an English tokenizer test.\"\n",
        "print(f\"\\nOriginal English: {sample_english_sentence}\")\n",
        "tokenized_tgt_sample = tokenizer_tgt(sample_english_sentence)\n",
        "print(f\"Tokens: {tokenizer_tgt.tokenize(sample_english_sentence)}\")\n",
        "print(f\"Token IDs: {tokenized_tgt_sample['input_ids']}\")\n",
        "print(f\"Decoded: {tokenizer_tgt.decode(tokenized_tgt_sample['input_ids'])}\")\n",
        "\n",
        "# Vocabulary sizes\n",
        "print(f\"\\nSource (Korean) tokenizer vocab size: {tokenizer_src.vocab_size}\")\n",
        "print(f\"Target (English) tokenizer vocab size: {tokenizer_tgt.vocab_size}\")\n",
        "print(f\"Source PAD ID: {tokenizer_src.pad_token_id}, CLS ID: {tokenizer_src.cls_token_id}, SEP ID: {tokenizer_src.sep_token_id}\")\n",
        "print(f\"Target PAD ID: {tokenizer_tgt.pad_token_id}, CLS ID: {tokenizer_tgt.cls_token_id}, SEP ID: {tokenizer_tgt.sep_token_id}\")"
      ],
      "id": "_eCbZ9hhV97x"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHI1WkIFV97x"
      },
      "source": [
        "## 5. Creating a PyTorch Dataset 📦\n",
        "\n",
        "PyTorch's `Dataset` class provides an abstraction over our data. We'll create a custom `TranslationDataset` that will:\n",
        "1.  Take a DataFrame (train, val, or test) and our tokenizers.\n",
        "2.  In its `__getitem__` method, fetch a Korean-English pair.\n",
        "3.  Tokenize them.\n",
        "4.  Return the tokenized source sentence, tokenized target sentence (for decoder input), and labels (also from target sentence, for loss calculation).\n",
        "\n",
        "For a Seq2Seq model, typical inputs/outputs per sample are:\n",
        "-   `encoder_input_ids`: Tokenized source sentence.\n",
        "-   `decoder_input_ids`: Tokenized target sentence, usually starting with a start-of-sequence (SOS) token (e.g., `[CLS]`). This is fed to the decoder during training (teacher forcing).\n",
        "-   `labels`: Tokenized target sentence, usually ending with an end-of-sequence (EOS) token (e.g., `[SEP]`). This is what the decoder aims to predict. Often, labels are a shifted version of `decoder_input_ids`.\n",
        "\n",
        "Our `BertTokenizerFast` automatically adds `[CLS]` (start) and `[SEP]` (end) tokens."
      ],
      "id": "QHI1WkIFV97x"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UmJN5BY6V97x",
        "outputId": "c95e5ea7-07c4-4150-8365-e286728b9fd2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample from TranslationDataset:\n",
            "  Source IDs: tensor([    2,    11,    70,  4360,  4551, 13306,    71, 12901,  9564, 12435,\n",
            "           11,  3546, 14567,  4326,  8934,  8407,  7400,  4154,  3252,  6420,\n",
            "        12985,  5025,  3397,  6461,    18,     3])\n",
            "  Source Decoded: [CLS]'bible coloring'은 성경의 아름다운 이야기를 체험 할 수 있는 컬러링 앱입니다. [SEP]\n",
            "  Target (Decoder Input) IDs: tensor([    2, 26268, 23067,    11,  1056,    69, 23067,  2803,  1067,  5155,\n",
            "         1117,  1042,  2405,  4024,  5520,  1039,  1023, 26268,    18])\n",
            "  Target (Decoder Input) Decoded: [CLS] bible coloring'is a coloring application that allows you to experience beautiful stories in the bible.\n",
            "  Labels IDs: tensor([26268, 23067,    11,  1056,    69, 23067,  2803,  1067,  5155,  1117,\n",
            "         1042,  2405,  4024,  5520,  1039,  1023, 26268,    18,     3])\n",
            "  Labels Decoded: bible coloring'is a coloring application that allows you to experience beautiful stories in the bible. [SEP]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, dataframe, src_tokenizer, tgt_tokenizer, src_col=\"원문\", tgt_col=\"번역문\", max_length=128):\n",
        "        self.dataframe = dataframe\n",
        "        self.src_tokenizer = src_tokenizer\n",
        "        self.tgt_tokenizer = tgt_tokenizer\n",
        "        self.src_col = src_col\n",
        "        self.tgt_col = tgt_col\n",
        "        self.max_length = max_length # Max sequence length for truncation\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src_text = str(self.dataframe.iloc[idx][self.src_col])\n",
        "        tgt_text = str(self.dataframe.iloc[idx][self.tgt_col])\n",
        "\n",
        "        # Tokenize source sentence\n",
        "        # `encode` returns a list of IDs. We convert to tensor.\n",
        "        # `add_special_tokens=True` adds [CLS] and [SEP]\n",
        "        encoder_input_ids = torch.tensor(\n",
        "            self.src_tokenizer.encode(src_text, add_special_tokens=True, truncation=True, max_length=self.max_length)\n",
        "        )\n",
        "\n",
        "        # Tokenize target sentence for both decoder input and labels\n",
        "        # For many Transformer-based Seq2Seq, decoder_input and labels can be the same tokenized target sequence.\n",
        "        # The causal attention mask in the decoder ensures it only attends to previous positions.\n",
        "        # For RNNs, decoder_input is often <SOS> + target_tokens and labels are target_tokens + <EOS>.\n",
        "        # Since our tokenizers add [CLS] and [SEP], we can use the tokenized sequence directly.\n",
        "        target_token_ids = torch.tensor(\n",
        "            self.tgt_tokenizer.encode(tgt_text, add_special_tokens=True, truncation=True, max_length=self.max_length)\n",
        "        )\n",
        "\n",
        "        # In the original notebook, pack_collate expects (source, target, shifted_target)\n",
        "        # Let's prepare them such that:\n",
        "        # - source = encoder_input_ids\n",
        "        # - target = decoder_input_ids (e.g. [CLS] w1 w2)\n",
        "        # - shifted_target = labels (e.g. w1 w2 [SEP])\n",
        "        decoder_input_ids = target_token_ids[:-1]\n",
        "        labels = target_token_ids[1:]\n",
        "\n",
        "        return encoder_input_ids, decoder_input_ids, labels\n",
        "\n",
        "# Create Dataset instances\n",
        "MAX_SEQ_LENGTH = 100 # Define a max length for sequences\n",
        "dataset = TranslationDataset(df, tokenizer_src, tokenizer_tgt, max_length=MAX_SEQ_LENGTH)\n",
        "# Test a sample from the dataset\n",
        "sample_src_ids, sample_tgt_ids, sample_lbl_ids = dataset[0]\n",
        "print(\"Sample from TranslationDataset:\")\n",
        "print(f\"  Source IDs: {sample_src_ids}\")\n",
        "print(f\"  Source Decoded: {tokenizer_src.decode(sample_src_ids)}\")\n",
        "print(f\"  Target (Decoder Input) IDs: {sample_tgt_ids}\")\n",
        "print(f\"  Target (Decoder Input) Decoded: {tokenizer_tgt.decode(sample_tgt_ids)}\")\n",
        "print(f\"  Labels IDs: {sample_lbl_ids}\")\n",
        "print(f\"  Labels Decoded: {tokenizer_tgt.decode(sample_lbl_ids)}\")"
      ],
      "id": "UmJN5BY6V97x"
    },
    {
      "cell_type": "markdown",
      "id": "2f86fbd3",
      "metadata": {
        "id": "2f86fbd3"
      },
      "source": [
        "## 6. Splitting the Dataset\n",
        "\n",
        "To train and evaluate our model robustly, we split our data into three sets:\n",
        "1.  **Training set**: Used to train the model parameters.\n",
        "2.  **Validation set**: Used during training to monitor performance, tune hyperparameters, and prevent overfitting.\n",
        "3.  **Test set**: Used *only once* at the very end to get an unbiased evaluation of the final model.\n",
        "\n",
        "We'll use a common split (e.g., 80% train, 10% validation, 10% test). For faster execution in this live session, we might use a subset of the full data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bab4d36",
      "metadata": {
        "id": "2bab4d36",
        "outputId": "6adbf373-2595-4f41-e941-7c37676baf8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train dataset size: 1281935\n",
            "Validation dataset size: 160242\n",
            "Test dataset size: 160241\n"
          ]
        }
      ],
      "source": [
        "train_ratio = 0.8\n",
        "val_ratio = 0.1\n",
        "test_ratio = 0.1\n",
        "\n",
        "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_ratio, val_ratio, 1.0 - train_ratio - val_ratio], generator=torch.Generator().manual_seed(42))\n",
        "\n",
        "print(f\"Train dataset size: {len(train_dataset)}\")\n",
        "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
        "print(f\"Test dataset size: {len(test_dataset)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELiaX4MCV97y"
      },
      "source": [
        "## 7. DataLoader and Collate Function for Variable Length Sequences 🔄\n",
        "\n",
        "Sentences have different lengths. When creating batches of data with `DataLoader`, we need to handle these varying lengths.\n",
        "-   **Padding**: Making all sequences in a batch the same length by adding `[PAD]` tokens.\n",
        "-   **Packing**: More efficient for RNNs. `torch.nn.utils.rnn.pack_sequence` sorts sequences by length, concatenates non-padded elements, and stores `batch_sizes` (how many sequences are active at each timestep). PyTorch RNNs can process `PackedSequence` efficiently.\n",
        "\n",
        "We'll define a **collate function**. This function is passed to `DataLoader` and takes a list of samples (from our `Dataset`) to form a batch. Our collate function will use `pack_sequence`."
      ],
      "id": "ELiaX4MCV97y"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kaOySvA-V97y"
      },
      "outputs": [],
      "source": [
        "from torch.nn.utils.rnn import pack_sequence, PackedSequence\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def pack_collate_fn(raw_batch_list):\n",
        "    # raw_batch_list is a list of tuples: [(src1, tgt_in1, lbl1), (src2, tgt_in2, lbl2), ...]\n",
        "    # Each src_i, tgt_in_i, lbl_i is a 1D tensor of token IDs.\n",
        "\n",
        "    sources, target_inputs, labels = zip(*raw_batch_list)\n",
        "    # Now, sources = (src1, src2, ...), target_inputs = (tgt_in1, tgt_in2, ...), etc.\n",
        "\n",
        "    # `pack_sequence` expects a list of Tensors. It will sort them by length (descending)\n",
        "    # if enforce_sorted=False (which is the default).\n",
        "    packed_sources = pack_sequence(sources, enforce_sorted=False)\n",
        "    packed_target_inputs = pack_sequence(target_inputs, enforce_sorted=False)\n",
        "    packed_labels = pack_sequence(labels, enforce_sorted=False)\n",
        "\n",
        "    return packed_sources, packed_target_inputs, packed_labels\n",
        "\n",
        "# Create DataLoaders\n",
        "BATCH_SIZE = 64 # Adjust based on GPU memory\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, collate_fn=pack_collate_fn, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, collate_fn=pack_collate_fn, shuffle=False, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, collate_fn=pack_collate_fn, shuffle=False, num_workers=2)\n",
        "\n",
        "# Let's test the DataLoader and collate function\n",
        "print(\"Testing DataLoader and pack_collate_fn...\")\n",
        "try:\n",
        "    batch_src_packed, batch_tgt_in_packed, batch_lbl_packed = next(iter(train_loader))\n",
        "    print(\"Batch loaded successfully!\")\n",
        "\n",
        "    print(\"\\nSource Batch (PackedSequence):\")\n",
        "    print(f\"  Data shape (flattened tokens): {batch_src_packed.data.shape}\")\n",
        "    print(f\"  Batch sizes (active sequences per timestep): {batch_src_packed.batch_sizes}\")\n",
        "    # print(f\"  Sorted indices (original pos of sorted seqs): {batch_src_packed.sorted_indices}\")\n",
        "    # print(f\"  Unsorted indices (how to restore original order): {batch_src_packed.unsorted_indices}\")\n",
        "\n",
        "    print(\"\\nTarget Input Batch (PackedSequence):\")\n",
        "    print(f\"  Data shape: {batch_tgt_in_packed.data.shape}\")\n",
        "    print(f\"  Batch sizes: {batch_tgt_in_packed.batch_sizes}\")\n",
        "\n",
        "    print(\"\\nLabels Batch (PackedSequence):\")\n",
        "    print(f\"  Data shape: {batch_lbl_packed.data.shape}\")\n",
        "    print(f\"  Batch sizes: {batch_lbl_packed.batch_sizes}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error loading batch: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ],
      "id": "kaOySvA-V97y"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zP9QBszV97y"
      },
      "source": [
        "## 8. Defining the Sequence-to-Sequence (Seq2Seq) Model 🧠\n",
        "\n",
        "We'll build a classic Encoder-Decoder model using GRU (Gated Recurrent Unit) layers.\n",
        "\n",
        "![Seq2Seq Architecture](https://raw.githubusercontent.com/tensorflow/nmt/master/nmt/g3doc/img/seq2seq.jpg)\n",
        "*(Image source: TensorFlow NMT Tutorial)*\n",
        "\n",
        "**Encoder**:\n",
        "1.  **Embedding Layer**: Converts input source tokens (Korean) into dense vectors. We'll use `padding_idx` so PAD tokens have a zero embedding and don't contribute to gradients.\n",
        "2.  **GRU Layer**: Processes the sequence of embeddings and outputs all hidden states and the final hidden state (context vector).\n",
        "\n",
        "**Decoder**:\n",
        "1.  **Embedding Layer**: Converts input target tokens (English) into dense vectors.\n",
        "2.  **GRU Layer**: Takes the current target token's embedding and the previous hidden state (initialized with the encoder's context vector) to generate an output.\n",
        "3.  **Linear Layer (Projection)**: Maps the GRU's output to logits over the target vocabulary.\n",
        "\n",
        "Our model components will handle `PackedSequence` inputs. The encoder's final hidden state must be correctly passed to the decoder, potentially reordering it if the source and target sequences within a batch were sorted differently by `pack_sequence`."
      ],
      "id": "4zP9QBszV97y"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f4mI2FguV97y"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import PackedSequence # Already imported but good for clarity\n",
        "\n",
        "import torch.nn as nn\n",
        "class Seq2seq(nn.Module):\n",
        "  def __init__(self, enc_vocab, dec_vocab, hidden_size, num_layers=2):\n",
        "    super().__init__()\n",
        "    self.encoder = Encoder(enc_vocab, hidden_size, num_layers=num_layers)\n",
        "    self.decoder = Decoder(dec_vocab, hidden_size, num_layers=num_layers)\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self, num_vocab, hidden_size, num_layers=2):\n",
        "    super().__init__()\n",
        "    self.emb = nn.Embedding(num_vocab, hidden_size)\n",
        "    self.rnn = nn.GRU(hidden_size, hidden_size, batch_first=True, num_layers=num_layers)\n",
        "    # batch_first True: it takes (Num_samples_in_batch, num_timesteps, num_dim)\n",
        "    # batch_first False: it takes (num_timesteps, Num_samples_in_batch, num_dim)\n",
        "\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self, num_vocab, hidden_size, num_layers=2):\n",
        "    super().__init__()\n",
        "    self.emb = nn.Embedding(num_vocab, hidden_size)\n",
        "    self.rnn = nn.GRU(hidden_size, hidden_size, batch_first=True, num_layers=num_layers)\n",
        "    self.proj = nn.Linear(hidden_size, num_vocab)\n",
        "\n"
      ],
      "id": "f4mI2FguV97y"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1395db77",
      "metadata": {
        "id": "1395db77"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c32b22e",
      "metadata": {
        "id": "5c32b22e"
      },
      "outputs": [],
      "source": [
        "\n",
        "# --- Model Hyperparameters & Instantiation ---\n",
        "EMBEDDING_DIM = 256\n",
        "HIDDEN_DIM = 512  # Hidden dimension for GRU\n",
        "NUM_LAYERS = 2    # Number of GRU layers\n",
        "DROPOUT_P = 0.3   # Dropout probability\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Instantiate model components\n",
        "enc = Encoder(tokenizer_src.vocab_size, EMBEDDING_DIM, HIDDEN_DIM, NUM_LAYERS, DROPOUT_P, tokenizer_src.pad_token_id).to(device)\n",
        "dec = Decoder(tokenizer_tgt.vocab_size, EMBEDDING_DIM, HIDDEN_DIM, NUM_LAYERS, DROPOUT_P, tokenizer_tgt.pad_token_id).to(device)\n",
        "model = Seq2Seq(enc, dec, device).to(device)\n",
        "\n",
        "print(f\"Model created with {sum(p.numel() for p in model.parameters() if p.requires_grad):,} trainable parameters.\")\n",
        "\n",
        "# Test forward pass with a batch (if batch variables exist from DataLoader test)\n",
        "if 'batch_src_packed' in locals() and 'batch_tgt_in_packed' in locals():\n",
        "    print(\"\\nTesting model forward pass with a sample batch...\")\n",
        "    try:\n",
        "        # Ensure batch tensors are on the correct device\n",
        "        src_dev = PackedSequence(batch_src_packed.data.to(device), batch_src_packed.batch_sizes, batch_src_packed.sorted_indices, batch_src_packed.unsorted_indices)\n",
        "        tgt_in_dev = PackedSequence(batch_tgt_in_packed.data.to(device), batch_tgt_in_packed.batch_sizes, batch_tgt_in_packed.sorted_indices, batch_tgt_in_packed.unsorted_indices)\n",
        "\n",
        "        model.train() # Set to train mode for consistent behavior if dropout/batchnorm were used differently\n",
        "        output_logits_packed = model(src_dev, tgt_in_dev)\n",
        "\n",
        "        print(\"Model forward pass successful!\")\n",
        "        print(f\"Output (PackedSequence logits data shape): {output_logits_packed.data.shape}\")\n",
        "        # Expected: (total_num_target_tokens_in_batch, target_vocab_size)\n",
        "    except Exception as e:\n",
        "        print(f\"Error during model forward pass test: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "else:\n",
        "    print(\"\\nSkipping model forward pass test as batch data is not loaded. Run DataLoader cell first.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afnQnU-jV97y"
      },
      "source": [
        "## 9. Training the Model 🔥\n",
        "\n",
        "The training process involves iterating through the training dataset in epochs and batches:\n",
        "1.  **Get batch**: From `train_loader`.\n",
        "2.  **Zero gradients**: `optimizer.zero_grad()`.\n",
        "3.  **Forward pass**: `model(source_batch, target_input_batch)` to get `output_logits`.\n",
        "4.  **Calculate loss**: Compare `output_logits.data` with `labels_batch.data`. `CrossEntropyLoss` is suitable, and its `ignore_index` parameter should be set to the PAD token ID for the target language so padding doesn't contribute to the loss.\n",
        "5.  **Backward pass**: `loss.backward()` to compute gradients.\n",
        "6.  **Gradient clipping**: (Optional but recommended) `torch.nn.utils.clip_grad_norm_` to prevent exploding gradients.\n",
        "7.  **Optimizer step**: `optimizer.step()` to update model weights.\n",
        "\n",
        "We'll also define an evaluation function to check performance on the validation set.\n",
        "\n",
        "**Note**: Full training takes time. For this live session, we'll define the loop structure and then load pre-trained weights."
      ],
      "id": "afnQnU-jV97y"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-1qR0UWrV97y"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "import time\n",
        "import math\n",
        "\n",
        "# Define Loss Function\n",
        "# The output of our model (PackedSequence.data) is [total_target_tokens, target_vocab_size]\n",
        "# The labels (PackedSequence.data) are [total_target_tokens]\n",
        "# This is exactly what CrossEntropyLoss expects.\n",
        "TARGET_PAD_IDX = tokenizer_tgt.pad_token_id\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=TARGET_PAD_IDX)\n",
        "\n",
        "# Define Optimizer\n",
        "LEARNING_RATE = 1e-3 # 0.001\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# Training configuration\n",
        "N_EPOCHS = 1 # Set higher for actual training (e.g., 10-20)\n",
        "CLIP = 1.0     # Gradient clipping value\n",
        "\n",
        "def train_epoch(model, dataloader, optimizer, criterion, clip, device):\n",
        "    model.train() # Set model to training mode\n",
        "    epoch_loss = 0\n",
        "    num_batches = len(dataloader)\n",
        "\n",
        "    for i, (src_packed, tgt_in_packed, lbl_packed) in enumerate(dataloader):\n",
        "        # Move batch data to the device\n",
        "        src_dev = PackedSequence(src_packed.data.to(device), src_packed.batch_sizes, src_packed.sorted_indices, src_packed.unsorted_indices)\n",
        "        tgt_in_dev = PackedSequence(tgt_in_packed.data.to(device), tgt_in_packed.batch_sizes, tgt_in_packed.sorted_indices, tgt_in_packed.unsorted_indices)\n",
        "        lbl_dev_data = lbl_packed.data.to(device) # Only need .data for labels\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass: output_logits_packed.data is (sum_lengths, vocab_size)\n",
        "        output_logits_packed = model(src_dev, tgt_in_dev)\n",
        "\n",
        "        # Calculate loss: output_logits_packed.data vs lbl_dev_data\n",
        "        # output_logits_packed.data shape: (total_tokens_in_batch, target_vocab_size)\n",
        "        # lbl_dev_data shape: (total_tokens_in_batch)\n",
        "        loss = criterion(output_logits_packed.data, lbl_dev_data)\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip) # Clip gradients\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        if (i + 1) % (num_batches // 10 if num_batches >= 10 else 1) == 0: # Print progress ~10 times\n",
        "            print(f'  Batch {i+1}/{num_batches} | Train Loss: {loss.item():.3f}')\n",
        "\n",
        "    return epoch_loss / num_batches\n",
        "\n",
        "def evaluate_epoch(model, dataloader, criterion, device):\n",
        "    model.eval() # Set model to evaluation mode\n",
        "    epoch_loss = 0\n",
        "    with torch.no_grad(): # Disable gradient calculations\n",
        "        for i, (src_packed, tgt_in_packed, lbl_packed) in enumerate(dataloader):\n",
        "            src_dev = PackedSequence(src_packed.data.to(device), src_packed.batch_sizes, src_packed.sorted_indices, src_packed.unsorted_indices)\n",
        "            tgt_in_dev = PackedSequence(tgt_in_packed.data.to(device), tgt_in_packed.batch_sizes, tgt_in_packed.sorted_indices, tgt_in_packed.unsorted_indices)\n",
        "            lbl_dev_data = lbl_packed.data.to(device)\n",
        "\n",
        "            output_logits_packed = model(src_dev, tgt_in_dev)\n",
        "            loss = criterion(output_logits_packed.data, lbl_dev_data)\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(dataloader)\n",
        "\n",
        "print(\"Training and evaluation loop structures defined.\")\n",
        "print(\"We will skip actual training and load pre-trained weights for the live session.\")\n",
        "\n",
        "# --- Example of starting a training loop (commented out for live coding) ---\n",
        "# best_valid_loss = float('inf')\n",
        "# MODEL_SAVE_PATH = 'best-seq2seq-model.pt'\n",
        "\n",
        "# for epoch in range(N_EPOCHS):\n",
        "#     start_time = time.time()\n",
        "#     print(f'Starting Epoch: {epoch+1:02}/{N_EPOCHS}')\n",
        "\n",
        "#     train_loss = train_epoch(model, train_loader, optimizer, criterion, CLIP, device)\n",
        "#     valid_loss = evaluate_epoch(model, val_loader, criterion, device)\n",
        "\n",
        "#     end_time = time.time()\n",
        "#     epoch_mins, epoch_secs = divmod(end_time - start_time, 60)\n",
        "\n",
        "#     if valid_loss < best_valid_loss:\n",
        "#         best_valid_loss = valid_loss\n",
        "#         torch.save({\n",
        "#             'epoch': epoch,\n",
        "#             'model_state_dict': model.state_dict(),\n",
        "#             'optimizer_state_dict': optimizer.state_dict(),\n",
        "#             'loss': best_valid_loss,\n",
        "#             }, MODEL_SAVE_PATH)\n",
        "#         print(f'  ** Best validation loss: {best_valid_loss:.3f}. Model saved to {MODEL_SAVE_PATH} **')\n",
        "\n",
        "#     print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {int(epoch_secs)}s')\n",
        "#     print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "#     print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ],
      "id": "-1qR0UWrV97y"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g54Zwb68V97y"
      },
      "source": [
        "## 10. Loading Pre-trained Weights 💾\n",
        "\n",
        "Training a good NMT model from scratch can take many hours or days. To save time, we'll load pre-trained weights into our model structure.\n",
        "\n",
        "**Important**: The architecture of the model we define (embedding dimensions, hidden sizes, number of layers, vocabulary sizes) *must exactly match* the architecture used when these weights were saved.\n",
        "\n",
        "The original notebook used `hidden_size = 512` and `num_layers=3` for its pre-trained weights. Let's define a new model instance with these parameters to load the weights."
      ],
      "id": "g54Zwb68V97y"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5L2pWh6-V97y"
      },
      "outputs": [],
      "source": [
        "# Download the pre-trained model weights\n",
        "!gdown 15jL2TaRk6Q47uuPWDruUge6O_gCPv5mp --quiet -O kor_eng_translator_model_vanilla_best.pt\n",
        "print(\"Pre-trained weights 'kor_eng_translator_model_vanilla_best.pt' downloaded.\")\n",
        "\n",
        "# Parameters matching the pre-trained model (from original notebook's loading cell)\n",
        "PRETRAINED_EMBEDDING_DIM = EMBEDDING_DIM # Assuming embedding dim was consistent, adjust if needed\n",
        "PRETRAINED_HIDDEN_DIM = 512\n",
        "PRETRAINED_NUM_LAYERS = 3\n",
        "PRETRAINED_DROPOUT_P = DROPOUT_P # Assuming dropout was consistent\n",
        "\n",
        "print(f\"\\nInstantiating model for pre-trained weights:\")\n",
        "print(f\"  Embedding Dim: {PRETRAINED_EMBEDDING_DIM}\")\n",
        "print(f\"  Hidden Dim: {PRETRAINED_HIDDEN_DIM}\")\n",
        "print(f\"  Num Layers: {PRETRAINED_NUM_LAYERS}\")\n",
        "print(f\"  Dropout: {PRETRAINED_DROPOUT_P}\")\n",
        "print(f\"  Source Vocab Size: {tokenizer_src.vocab_size}\")\n",
        "print(f\"  Target Vocab Size: {tokenizer_tgt.vocab_size}\")\n",
        "\n",
        "\n",
        "# Instantiate the model with parameters matching the pre-trained file\n",
        "enc_loaded = Encoder(tokenizer_src.vocab_size, PRETRAINED_EMBEDDING_DIM, PRETRAINED_HIDDEN_DIM,\n",
        "                     PRETRAINED_NUM_LAYERS, PRETRAINED_DROPOUT_P, tokenizer_src.pad_token_id).to(device)\n",
        "dec_loaded = Decoder(tokenizer_tgt.vocab_size, PRETRAINED_EMBEDDING_DIM, PRETRAINED_HIDDEN_DIM,\n",
        "                     PRETRAINED_NUM_LAYERS, PRETRAINED_DROPOUT_P, tokenizer_tgt.pad_token_id).to(device)\n",
        "model_loaded = Seq2Seq(enc_loaded, dec_loaded, device).to(device)\n",
        "\n",
        "try:\n",
        "    # The checkpoint file contains a dictionary, and the model's state_dict is under the 'model' key.\n",
        "    checkpoint = torch.load(\"kor_eng_translator_model_vanilla_best.pt\", map_location=device)\n",
        "    model_loaded.load_state_dict(checkpoint['model'])\n",
        "    print(\"\\nPre-trained model weights loaded successfully!\")\n",
        "    print(f\"Loaded model has {sum(p.numel() for p in model_loaded.parameters()):,} total parameters.\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nError loading pre-trained weights: {e}\")\n",
        "    print(\"Ensure the model architecture (vocab sizes, dimensions, layers) matches the saved state_dict.\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ],
      "id": "5L2pWh6-V97y"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYTQe5dnV97z"
      },
      "source": [
        "## 11. Translation (Inference) 🗣️➡️💬\n",
        "\n",
        "With our (pre-trained) model ready, let's translate some new Korean sentences into English!\n",
        "The inference process (also called decoding) works step-by-step:\n",
        "1.  Tokenize the input Korean sentence.\n",
        "2.  Pass tokenized input through the **Encoder** to get the context vector (final hidden state).\n",
        "3.  Initialize the **Decoder** with this context vector.\n",
        "4.  Start decoding with a special start-of-sequence (SOS) token (e.g., `[CLS]`).\n",
        "5.  In a loop, for each step:\n",
        "    a.  Feed the current generated token (or SOS for the first step) and the previous decoder hidden state to the decoder.\n",
        "    b.  Get the logits (raw scores) over the target vocabulary.\n",
        "    c.  Select the token with the highest score (this is called **greedy decoding**).\n",
        "    d.  If the selected token is an end-of-sequence (EOS) token (e.g., `[SEP]`), stop.\n",
        "    e.  Otherwise, add the token to our list of output tokens and use it as input for the next decoding step.\n",
        "6.  Convert the list of output token IDs back into an English sentence."
      ],
      "id": "PYTQe5dnV97z"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QNYlZYC8V97z"
      },
      "outputs": [],
      "source": [],
      "id": "QNYlZYC8V97z"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}