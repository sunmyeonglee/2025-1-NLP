{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sunmyeonglee/2025-1-NLP/blob/main/2_named_entity_recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45a2da87"
      },
      "source": [
        "# Named Entity Recognition\n",
        "- For a given word and its context window, estimate whether the given word is location or not"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9VN9TB17Z-U"
      },
      "source": [
        "## Introduction to Named Entity Recognition (NER)\n",
        "\n",
        "Named Entity Recognition is a subtask of Natural Language Processing (NLP) that focuses on identifying and categorizing named entities in text. Named entities are real-world objects such as people, organizations, locations, dates, etc. that can be denoted with a proper name.\n",
        "\n",
        "In this notebook, we'll focus on a binary classification task: determining whether a word represents an organization or not based on its surrounding context.\n",
        "\n",
        "We'll go through the following steps:\n",
        "1. Download and explore the CoNLL2003 dataset\n",
        "2. Preprocess the data and create context windows\n",
        "3. Convert words to vectors using pre-trained word embeddings\n",
        "4. Build and train a neural network classifier\n",
        "5. Evaluate the model on test data\n",
        "6. Use the model for inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11619f5d"
      },
      "source": [
        "# 1. Download dataset\n",
        "- CoNLL2003"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzOblyOl7Z-V"
      },
      "source": [
        "The CoNLL2003 dataset is a widely used benchmark for NER tasks. It contains news articles with manually annotated named entities. Each word is labeled with its entity type (if any): person (PER), location (LOC), organization (ORG), or miscellaneous (MISC).\n",
        "\n",
        "Let's download the dataset and extract it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "10a4faa3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c553fde-db45-4921-e39e-b3ad17530a4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-08 04:47:40--  https://data.deepai.org/conll2003.zip\n",
            "Resolving data.deepai.org (data.deepai.org)... 84.17.38.227, 2400:52e0:1500::1024:1\n",
            "Connecting to data.deepai.org (data.deepai.org)|84.17.38.227|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 982975 (960K) [application/zip]\n",
            "Saving to: ‘conll2003.zip.2’\n",
            "\n",
            "conll2003.zip.2     100%[===================>] 959.94K  3.47MB/s    in 0.3s    \n",
            "\n",
            "2025-04-08 04:47:41 (3.47 MB/s) - ‘conll2003.zip.2’ saved [982975/982975]\n",
            "\n",
            "Archive:  conll2003.zip\n",
            "replace metadata? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace test.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace train.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace valid.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n"
          ]
        }
      ],
      "source": [
        "!wget https://data.deepai.org/conll2003.zip # Download dataset\n",
        "!unzip conll2003.zip # Unzip dataset zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7643dde5"
      },
      "source": [
        "## 2. Preprocess Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqbmxSb47Z-V"
      },
      "source": [
        "Now that we have the dataset, let's first take a look at its structure. Each line in the dataset contains a word followed by its part-of-speech tag, syntactic chunk tag, and named entity tag. We'll focus on the words and their named entity tags."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "d31874b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7cd07d0-522b-4105-95f0-97faf81417db"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['-DOCSTART- -X- -X- O',\n",
              " '',\n",
              " 'EU NNP B-NP B-ORG',\n",
              " 'rejects VBZ B-VP O',\n",
              " 'German JJ B-NP B-MISC',\n",
              " 'call NN I-NP O',\n",
              " 'to TO B-VP O',\n",
              " 'boycott VB I-VP O',\n",
              " 'British JJ B-NP B-MISC',\n",
              " 'lamb NN I-NP O',\n",
              " '. . O O',\n",
              " '',\n",
              " 'Peter NNP B-NP B-PER',\n",
              " 'Blackburn NNP I-NP I-PER',\n",
              " '',\n",
              " 'BRUSSELS NNP B-NP B-LOC',\n",
              " '1996-08-22 CD I-NP O',\n",
              " '',\n",
              " 'The DT B-NP O',\n",
              " 'European NNP I-NP B-ORG',\n",
              " 'Commission NNP I-NP I-ORG',\n",
              " 'said VBD B-VP O',\n",
              " 'on IN B-PP O',\n",
              " 'Thursday NNP B-NP O',\n",
              " 'it PRP B-NP O',\n",
              " 'disagreed VBD B-VP O',\n",
              " 'with IN B-PP O',\n",
              " 'German JJ B-NP B-MISC',\n",
              " 'advice NN I-NP O',\n",
              " 'to TO B-PP O',\n",
              " 'consumers NNS B-NP O',\n",
              " 'to TO B-VP O',\n",
              " 'shun VB I-VP O',\n",
              " 'British JJ B-NP B-MISC',\n",
              " 'lamb NN I-NP O',\n",
              " 'until IN B-SBAR O',\n",
              " 'scientists NNS B-NP O',\n",
              " 'determine VBP B-VP O',\n",
              " 'whether IN B-SBAR O',\n",
              " 'mad JJ B-NP O',\n",
              " 'cow NN I-NP O',\n",
              " 'disease NN I-NP O',\n",
              " 'can MD B-VP O',\n",
              " 'be VB I-VP O',\n",
              " 'transmitted VBN I-VP O',\n",
              " 'to TO B-PP O',\n",
              " 'sheep NN B-NP O',\n",
              " '. . O O',\n",
              " '',\n",
              " 'Germany NNP B-NP B-LOC',\n",
              " \"'s POS B-NP O\",\n",
              " 'representative NN I-NP O',\n",
              " 'to TO B-PP O',\n",
              " 'the DT B-NP O',\n",
              " 'European NNP I-NP B-ORG',\n",
              " 'Union NNP I-NP I-ORG',\n",
              " \"'s POS B-NP O\",\n",
              " 'veterinary JJ I-NP O',\n",
              " 'committee NN I-NP O',\n",
              " 'Werner NNP I-NP B-PER',\n",
              " 'Zwingmann NNP I-NP I-PER',\n",
              " 'said VBD B-VP O',\n",
              " 'on IN B-PP O',\n",
              " 'Wednesday NNP B-NP O',\n",
              " 'consumers NNS I-NP O',\n",
              " 'should MD B-VP O',\n",
              " 'buy VB I-VP O',\n",
              " 'sheepmeat NN B-NP O',\n",
              " 'from IN B-PP O',\n",
              " 'countries NNS B-NP O']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "with open(\"train.txt\") as f:\n",
        "  string = ''.join(f.readlines())\n",
        "dataset = string.split('\\n')\n",
        "\n",
        "dataset[:70]  # Display the first 70 lines of the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksflfQCc7Z-W"
      },
      "source": [
        "### 2.1 Organizing Text into Sentences\n",
        "\n",
        "The dataset contains sentences separated by empty lines. We'll group the words into sentences to maintain the context for each word. The `groupby` function from `itertools` helps us split the data based on empty lines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "49e7f34b",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cf9b191-f0ef-495c-fa67-0fea54dcd284"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['-DOCSTART- -X- -X- O'],\n",
              " ['EU NNP B-NP B-ORG',\n",
              "  'rejects VBZ B-VP O',\n",
              "  'German JJ B-NP B-MISC',\n",
              "  'call NN I-NP O',\n",
              "  'to TO B-VP O',\n",
              "  'boycott VB I-VP O',\n",
              "  'British JJ B-NP B-MISC',\n",
              "  'lamb NN I-NP O',\n",
              "  '. . O O'],\n",
              " ['Peter NNP B-NP B-PER', 'Blackburn NNP I-NP I-PER'],\n",
              " ['BRUSSELS NNP B-NP B-LOC', '1996-08-22 CD I-NP O'],\n",
              " ['The DT B-NP O',\n",
              "  'European NNP I-NP B-ORG',\n",
              "  'Commission NNP I-NP I-ORG',\n",
              "  'said VBD B-VP O',\n",
              "  'on IN B-PP O',\n",
              "  'Thursday NNP B-NP O',\n",
              "  'it PRP B-NP O',\n",
              "  'disagreed VBD B-VP O',\n",
              "  'with IN B-PP O',\n",
              "  'German JJ B-NP B-MISC',\n",
              "  'advice NN I-NP O',\n",
              "  'to TO B-PP O',\n",
              "  'consumers NNS B-NP O',\n",
              "  'to TO B-VP O',\n",
              "  'shun VB I-VP O',\n",
              "  'British JJ B-NP B-MISC',\n",
              "  'lamb NN I-NP O',\n",
              "  'until IN B-SBAR O',\n",
              "  'scientists NNS B-NP O',\n",
              "  'determine VBP B-VP O',\n",
              "  'whether IN B-SBAR O',\n",
              "  'mad JJ B-NP O',\n",
              "  'cow NN I-NP O',\n",
              "  'disease NN I-NP O',\n",
              "  'can MD B-VP O',\n",
              "  'be VB I-VP O',\n",
              "  'transmitted VBN I-VP O',\n",
              "  'to TO B-PP O',\n",
              "  'sheep NN B-NP O',\n",
              "  '. . O O']]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "from itertools import groupby\n",
        "\n",
        "dataset_in_sentence = [list(group) for k, group in groupby(dataset, lambda x: x == \"\") if not k]\n",
        "dataset_in_sentence[:5]  # Display the first 5 sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UzJBJd47Z-W"
      },
      "source": [
        "### 2.2 Filtering Short Sentences\n",
        "\n",
        "Some sentences in the dataset might be too short for our context window approach. Let's filter out sentences that have fewer than 6 words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "65c3dfa8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22d13284-bacc-4247-c410-8e87944159a6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10625"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# Filter out sentences that are too short for our context window approach\n",
        "filtered_dataset = [sentence for sentence in dataset_in_sentence if len(sentence) > 5]\n",
        "len(filtered_dataset)  # Number of sentences after filtering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "byACWZiOjr3l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b339f73-70e2-4851-9359-62eeba168d4f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\" \" O O',\n",
              " 'I PRP B-NP O',\n",
              " 'think VBP B-VP O',\n",
              " 'this DT B-NP O',\n",
              " 'is VBZ B-VP O',\n",
              " 'a DT B-NP O',\n",
              " 'bad JJ I-NP O',\n",
              " 'beginning NN I-NP O',\n",
              " '. . O O']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# Check an example sentence\n",
        "filtered_dataset[1000]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfPjb0sr7Z-W"
      },
      "source": [
        "### 2.3 Creating Context Windows\n",
        "\n",
        "For each word in a sentence, we'll create a context window that includes the target word and its surrounding words. This context will help our model determine whether a word represents an organization.\n",
        "\n",
        "Let's define a window length and see how the context window looks for each word in a sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "a1a1714b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9b5e70e-2e31-43b4-e51d-a037852981ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word is EU NNP B-NP B-ORG\n",
            "splitted_word is ['EU', 'NNP', 'B-NP', 'B-ORG']\n",
            "EU B-ORG\n",
            "is organization: True\n",
            "prev_words is []\n",
            "next_words is ['rejects', 'German']\n",
            "concatenated_words is ['<pad>', '<pad>', 'EU', 'rejects', 'German']\n",
            "--------------------------------\n",
            "word is rejects VBZ B-VP O\n",
            "splitted_word is ['rejects', 'VBZ', 'B-VP', 'O']\n",
            "rejects O\n",
            "is organization: False\n",
            "prev_words is ['EU']\n",
            "next_words is ['German', 'call']\n",
            "concatenated_words is ['<pad>', 'EU', 'rejects', 'German', 'call']\n",
            "--------------------------------\n",
            "word is German JJ B-NP B-MISC\n",
            "splitted_word is ['German', 'JJ', 'B-NP', 'B-MISC']\n",
            "German B-MISC\n",
            "is organization: False\n",
            "prev_words is ['EU', 'rejects']\n",
            "next_words is ['call', 'to']\n",
            "concatenated_words is ['EU', 'rejects', 'German', 'call', 'to']\n",
            "--------------------------------\n",
            "word is call NN I-NP O\n",
            "splitted_word is ['call', 'NN', 'I-NP', 'O']\n",
            "call O\n",
            "is organization: False\n",
            "prev_words is ['rejects', 'German']\n",
            "next_words is ['to', 'boycott']\n",
            "concatenated_words is ['rejects', 'German', 'call', 'to', 'boycott']\n",
            "--------------------------------\n",
            "word is to TO B-VP O\n",
            "splitted_word is ['to', 'TO', 'B-VP', 'O']\n",
            "to O\n",
            "is organization: False\n",
            "prev_words is ['German', 'call']\n",
            "next_words is ['boycott', 'British']\n",
            "concatenated_words is ['German', 'call', 'to', 'boycott', 'British']\n",
            "--------------------------------\n",
            "word is boycott VB I-VP O\n",
            "splitted_word is ['boycott', 'VB', 'I-VP', 'O']\n",
            "boycott O\n",
            "is organization: False\n",
            "prev_words is ['call', 'to']\n",
            "next_words is ['British', 'lamb']\n",
            "concatenated_words is ['call', 'to', 'boycott', 'British', 'lamb']\n",
            "--------------------------------\n",
            "word is British JJ B-NP B-MISC\n",
            "splitted_word is ['British', 'JJ', 'B-NP', 'B-MISC']\n",
            "British B-MISC\n",
            "is organization: False\n",
            "prev_words is ['to', 'boycott']\n",
            "next_words is ['lamb', '.']\n",
            "concatenated_words is ['to', 'boycott', 'British', 'lamb', '.']\n",
            "--------------------------------\n",
            "word is lamb NN I-NP O\n",
            "splitted_word is ['lamb', 'NN', 'I-NP', 'O']\n",
            "lamb O\n",
            "is organization: False\n",
            "prev_words is ['boycott', 'British']\n",
            "next_words is ['.']\n",
            "concatenated_words is ['boycott', 'British', 'lamb', '.', '<pad>']\n",
            "--------------------------------\n",
            "word is . . O O\n",
            "splitted_word is ['.', '.', 'O', 'O']\n",
            ". O\n",
            "is organization: False\n",
            "prev_words is ['British', 'lamb']\n",
            "next_words is []\n",
            "concatenated_words is ['British', 'lamb', '.', '<pad>', '<pad>']\n",
            "--------------------------------\n"
          ]
        }
      ],
      "source": [
        "window_len = 2  # Number of words to consider on each side of the target word\n",
        "sentence = filtered_dataset[0]  # Example sentence\n",
        "\n",
        "for i, word in enumerate(sentence):\n",
        "  print(f'word is {word}')\n",
        "  splitted_word = word.split(' ')\n",
        "  print(f'splitted_word is {splitted_word}')\n",
        "  center_word = splitted_word[0]        # The actual word\n",
        "  label = splitted_word[-1]             # The entity tag (O, B-ORG, I-ORG, etc.)\n",
        "  print(center_word, label)\n",
        "  is_organization = label in ['B-ORG', 'I-ORG']  # Binary label: is this an organization?\n",
        "  print(f\"is organization: {is_organization}\")\n",
        "\n",
        "  # Collecting words to the left of the target word\n",
        "  prev_index = max(i - window_len, 0)  # Prevent negative indices\n",
        "  prev_words = sentence[prev_index:i]\n",
        "  prev_words = [word_str.split(' ')[0] for word_str in prev_words]  # Extract just the words\n",
        "\n",
        "  # Collecting words to the right of the target word\n",
        "  next_index = i + window_len + 1\n",
        "  next_words = sentence[i+1:next_index]\n",
        "  next_words = [word_str.split(' ')[0] for word_str in next_words]\n",
        "\n",
        "  print(f\"prev_words is {prev_words}\")\n",
        "  print(f\"next_words is {next_words}\")\n",
        "\n",
        "  # Add padding if there aren't enough words on either side\n",
        "  if len(prev_words) != window_len:\n",
        "    prev_words = ['<pad>'] * (window_len - len(prev_words)) + prev_words\n",
        "\n",
        "  if len(next_words) != window_len:\n",
        "    next_words = next_words + ['<pad>'] * (window_len - len(next_words))\n",
        "\n",
        "  # The final context window: previous words + target word + next words\n",
        "  concatenated_words = prev_words + [center_word] + next_words\n",
        "  print(f\"concatenated_words is {concatenated_words}\")\n",
        "  print(\"--------------------------------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNjz9IRP7Z-X"
      },
      "source": [
        "### 2.4 Creating Context Windows for All Sentences\n",
        "\n",
        "Now, let's create a function to generate context windows for all words in a sentence, and apply it to our entire dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "z1WVzcMGlZoh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "012a6842-ea6c-4f38-897d-9b06b6358948"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(['<pad>', '<pad>', 'EU', 'rejects', 'German'], True),\n",
              " (['<pad>', 'EU', 'rejects', 'German', 'call'], False),\n",
              " (['EU', 'rejects', 'German', 'call', 'to'], False),\n",
              " (['rejects', 'German', 'call', 'to', 'boycott'], False),\n",
              " (['German', 'call', 'to', 'boycott', 'British'], False),\n",
              " (['call', 'to', 'boycott', 'British', 'lamb'], False),\n",
              " (['to', 'boycott', 'British', 'lamb', '.'], False),\n",
              " (['boycott', 'British', 'lamb', '.', '<pad>'], False),\n",
              " (['British', 'lamb', '.', '<pad>', '<pad>'], False)]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "def make_window_words_and_label_from_sentence(sentence):\n",
        "  total_output = []\n",
        "  for i, word in enumerate(sentence):\n",
        "    splitted_word = word.split(' ')\n",
        "    center_word = splitted_word[0]\n",
        "    label = splitted_word[-1]\n",
        "    is_organization = label in ['B-ORG', 'I-ORG']\n",
        "\n",
        "    # Concatenating with neighboring words\n",
        "    # Words to the left\n",
        "    prev_index = max(i - window_len, 0)  # Clipping minimum to zero\n",
        "    prev_words = sentence[prev_index:i]\n",
        "    prev_words = [word_str.split(' ')[0] for word_str in prev_words]  # Collect the main word\n",
        "\n",
        "    # Words to the right\n",
        "    next_index = i + window_len + 1\n",
        "    next_words = sentence[i+1:next_index]\n",
        "    next_words = [word_str.split(' ')[0] for word_str in next_words]\n",
        "\n",
        "    # Add padding if needed\n",
        "    if len(prev_words) != window_len:\n",
        "      prev_words = ['<pad>'] * (window_len - len(prev_words)) + prev_words\n",
        "\n",
        "    if len(next_words) != window_len:\n",
        "      next_words = next_words + ['<pad>'] * (window_len - len(next_words))\n",
        "\n",
        "    concatenated_words = prev_words + [center_word] + next_words\n",
        "    total_output.append((concatenated_words, is_organization))\n",
        "  return total_output\n",
        "\n",
        "# Test the function on our example sentence\n",
        "make_window_words_and_label_from_sentence(sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZyER4rt7Z-X"
      },
      "source": [
        "### 2.5 Processing the Entire Dataset\n",
        "\n",
        "Now let's apply this function to all sentences in our dataset and flatten the result to get a list of (context_window, label) pairs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Ddb7t7PEmHfS"
      },
      "outputs": [],
      "source": [
        "# Process all sentences and flatten the result\n",
        "entire_dataset = [make_window_words_and_label_from_sentence(sentence) for sentence in filtered_dataset]\n",
        "# Flatten the nested list\n",
        "entire_dataset = [windowed_word for sentence in entire_dataset for windowed_word in sentence]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "gmQSEfE0mcOc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04e731ed-e8a0-4066-a675-491e52667880"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "192587"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "len(entire_dataset)  # Total number of words with context windows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "baTBawNXmg-s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c63946d-fe28-43f5-8904-0d863a3e0213"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['eight', 'in', 'a', 'row', ','], False)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "entire_dataset[10000]  # Example of a (context_window, label) pair"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VO2eMP4Y7Z-X"
      },
      "source": [
        "## 3. Word Embeddings\n",
        "\n",
        "To feed our text data into a neural network, we need to convert words into numerical vectors. We'll use pre-trained GloVe word embeddings, which capture semantic relationships between words based on their co-occurrence in a large corpus."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SiRIVGcBt7l1",
        "outputId": "2b49b590-cfa8-4156-a06d-bb794cfc3225"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "hPO83i3UKV8C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f1bbec3-c318-4808-f105-5e5e3aaa67fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 376.1/376.1MB downloaded\n"
          ]
        }
      ],
      "source": [
        "import gensim.downloader\n",
        "\n",
        "# Load pre-trained GloVe word embeddings (300-dimensional vectors)\n",
        "wrd2vec = gensim.downloader.load(\"glove-wiki-gigaword-300\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "BtVJXmXnLoN8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec7325f0-54e1-4279-cc51-f5c415270eaa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "400000"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "len(wrd2vec)  # Number of words in the vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "va_FJ6X67Z-X"
      },
      "source": [
        "### 3.1 Converting Words to Vectors\n",
        "\n",
        "We need to convert each context window into a single feature vector. We'll do this by:\n",
        "1. Converting each word in the context window to its vector representation\n",
        "2. Concatenating these vectors to form a single feature vector\n",
        "\n",
        "For words not in the GloVe vocabulary, we'll use zero vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "a0VrL0q2mvKB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13414361-f8ea-478e-ad6f-2e965f28f0e0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1500,)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "import numpy as np\n",
        "data_example = entire_dataset[0]\n",
        "word_list, label = data_example\n",
        "\n",
        "# Function to convert a list of words into a single concatenated vector\n",
        "def get_flattened_vector(word_list:list, wrd2vec):\n",
        "  cat_vector = []\n",
        "\n",
        "  for word in word_list:\n",
        "    if word.lower() in wrd2vec:\n",
        "      vector = wrd2vec[word.lower()]\n",
        "    else:\n",
        "      vector = np.zeros(300)\n",
        "    cat_vector.append(vector)\n",
        "  cat_vector = np.concatenate(cat_vector)\n",
        "  #print(cat_vector.shape)\n",
        "  return cat_vector\n",
        "\n",
        "# Check the shape of the resulting vector (5 words x 300 dimensions = 1500)\n",
        "get_flattened_vector(word_list, wrd2vec).shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word = word_list[3]\n",
        "wrd2vec[word].shape\n",
        "cat_vector = []\n",
        "\n",
        "for word in word_list:\n",
        "  if word.lower() in wrd2vec:\n",
        "    vector = wrd2vec[word.lower()]\n",
        "  else:\n",
        "    vector = np.zeros(300)\n",
        "  cat_vector.append(vector)\n",
        "cat_vector = np.concatenate(cat_vector)\n",
        "print(cat_vector.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUfaQJ4HD_3o",
        "outputId": "f63c6b75-2ea1-477a-9bce-bb3b369bb629"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1500,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2ATwsDO7Z-X"
      },
      "source": [
        "## 4. Building a Neural Network Classifier\n",
        "\n",
        "Now we'll create a simple neural network model to predict whether a word represents an organization based on its context. The model will take the concatenated word vectors as input and output a probability."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.layer1 = nn.Linear(in_features=1500, out_features=128)\n",
        "    self.layer2 = nn.Linear(in_features=128, out_features=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    layer1output = self.layer1(x)\n",
        "    layer2output = self.layer2(layer1output.relu())\n",
        "    return layer2output.sigmoid()\n",
        "  pass\n",
        "\n",
        "\n",
        "# Test the model with a sample input\n",
        "input_vec = get_flattened_vector(word_list, wrd2vec)\n",
        "model = Classifier()\n",
        "# model = Classifier(input_size=1500, hidden_size=47, output_size=1)\n",
        "model(torch.Tensor(input_vec))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmbxCvknJhLg",
        "outputId": "50d9bb16-1e40-480d-d2f7-df8375d9c49e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.5011], grad_fn=<SigmoidBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7hpYlLCuxIm"
      },
      "source": [
        "# 5. Creating a Dataset Class\n",
        "\n",
        "To make training more efficient, we'll create a custom Dataset class that handles data loading and preprocessing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3oX1SgN97Z-X"
      },
      "source": [
        "Our custom Dataset class will:\n",
        "1. Load and preprocess the CoNLL2003 dataset\n",
        "2. Create context windows for each word\n",
        "3. Convert words to vectors\n",
        "4. Provide an interface for accessing (feature, label) pairs\n",
        "\n",
        "This follows PyTorch's Dataset conventions, making it easy to use with DataLoader for batch processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o4PtExeIoVkh"
      },
      "outputs": [],
      "source": [
        "class Dataset:\n",
        "  def __init__(self, txt_fn=\"train.txt\", wrd2vec=None):\n",
        "    text_lines = self.load_txt_and_split(txt_fn)\n",
        "    dataset_in_sentence = self.group_by_sentence(text_lines)\n",
        "    filtered_dataset = [sentence for sentence in dataset_in_sentence if len(sentence) > 5]\n",
        "    entire_dataset = [self.make_window_words_and_label_from_sentence(sentence) for sentence in filtered_dataset]\n",
        "    self.entire_data = self.flatten_list_of_list(entire_dataset)\n",
        "    self.wrd2vec = wrd2vec\n",
        "\n",
        "  @staticmethod\n",
        "  def flatten_list_of_list(list_of_list): # in staticmehtod, we don't use self as an argument\n",
        "    return [y for x in list_of_list for y in x]\n",
        "    #  We don't use any of self.attribute or self.method() or something\n",
        "    # [ [1,2,3], [5,7,8], [0,1] ] -> [1,2,3,5,7,8,0,1]\n",
        "\n",
        "  @staticmethod\n",
        "  def make_window_words_and_label_from_sentence(sentence):\n",
        "    total_output = []\n",
        "    for i, word in enumerate(sentence):\n",
        "      splitted_word = word.split(' ')\n",
        "      center_word = splitted_word[0]\n",
        "      label = splitted_word[-1]\n",
        "      is_organization = label in ['B-ORG', 'I-ORG']\n",
        "\n",
        "      # concatenating with neighboring words\n",
        "      # words in the left\n",
        "      prev_index = max(i - window_len, 0)  # clipping minimum to zero\n",
        "      prev_words = sentence[prev_index:i]\n",
        "      prev_words = [word_str.split(' ')[0] for word_str in prev_words]  # collect the main word\n",
        "\n",
        "      # words in the right\n",
        "      next_index = i + window_len + 1\n",
        "      next_words = sentence[i+1:next_index]\n",
        "      next_words = [word_str.split(' ')[0] for word_str in next_words]\n",
        "\n",
        "      # Add padding if needed\n",
        "      if len(prev_words) != window_len:\n",
        "        prev_words = ['<pad>'] * (window_len - len(prev_words)) + prev_words\n",
        "\n",
        "      if len(next_words) != window_len:\n",
        "        next_words = next_words + ['<pad>'] * (window_len - len(next_words))\n",
        "\n",
        "      concatenated_words = prev_words + [center_word] + next_words\n",
        "      total_output.append((concatenated_words, is_organization))\n",
        "    return total_output\n",
        "\n",
        "  @staticmethod\n",
        "  def group_by_sentence(text_lines:list):\n",
        "    return [list(group) for k, group in groupby(text_lines, lambda x: x == \"\") if not k]\n",
        "\n",
        "  @staticmethod\n",
        "  def load_txt_and_split(txt_fn):\n",
        "    with open(txt_fn) as f:\n",
        "      string = ''.join(f.readlines())\n",
        "    dataset = string.split('\\n')\n",
        "    return dataset\n",
        "\n",
        "  def _load_txt_and_split(self, txt_fn=\"train.txt\"):\n",
        "    with open(txt_fn) as f:\n",
        "      string = ''.join(f.readlines())\n",
        "    dataset = string.split('\\n')\n",
        "    return dataset\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    data_sample = self.entire_data[idx]\n",
        "    windowed_words, label = data_sample\n",
        "    flattened_vector = self.get_flattened_vector(windowed_words, self.wrd2vec)\n",
        "    return flattened_vector, label\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.entire_data)\n",
        "\n",
        "  @staticmethod\n",
        "  def get_flattened_vector(word_list:list, wrd2vec):\n",
        "    flattened_vec = []\n",
        "    for word in word_list:\n",
        "      word = word.lower()\n",
        "      if word in wrd2vec:\n",
        "        vec = wrd2vec[word]\n",
        "      else:\n",
        "        vec = np.zeros(300)\n",
        "      flattened_vec.append(vec)\n",
        "    flattened_vec = np.concatenate(flattened_vec)\n",
        "    return flattened_vec\n",
        "\n",
        "# Create a dataset instance\n",
        "dataset = Dataset(wrd2vec=wrd2vec)\n",
        "dataset[0]  # Check a sample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDmn84317Z-Y"
      },
      "source": [
        "### 5.1 Creating a Data Loader\n",
        "\n",
        "Now we'll create a data loader that will efficiently batch our data for training. We need a custom collate function to handle our data format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WyD4aKwS0P54"
      },
      "outputs": [],
      "source": [
        "from typing import List, Tuple\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def custom_collate_fn(raw_batch:List[Tuple[np.ndarray, bool]]):\n",
        "  return\n",
        "\n",
        "# Create a data loader\n",
        "train_loader = DataLoader(dataset, batch_size=3, collate_fn=custom_collate_fn)\n",
        "\n",
        "# Test our custom collate function\n",
        "raw_batch = [dataset[i] for i in range(3)]  # This is how DataLoader calls its dataset\n",
        "custom_collate_fn(raw_batch)  # And it feeds this raw_batch to collate_fn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N58G8p9D7Z-Y"
      },
      "source": [
        "## 6. Training the Model\n",
        "\n",
        "Before training, we need to define a loss function. We'll use Binary Cross-Entropy (BCE) loss, which is appropriate for binary classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cZF_1rQo33fW"
      },
      "outputs": [],
      "source": [
        "def get_bce(pred, target, eps=1e-4):\n",
        "  pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDHfkCYN7Z-Y"
      },
      "source": [
        "Now let's train our model. We'll:\n",
        "1. Create the model with appropriate dimensions\n",
        "2. Define an optimizer (Adam)\n",
        "3. Train for multiple epochs\n",
        "4. Record the loss for plotting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zXMvJlDI3OAb"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Initialize model\n",
        "model = Classifier(input_size=1500, hidden_size=64, output_size=1)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)  # Adam optimizer\n",
        "train_loader = DataLoader(dataset, batch_size=128, collate_fn=custom_collate_fn, shuffle=True)\n",
        "dev = 'cuda'  # Use GPU if available\n",
        "\n",
        "model.to(dev)  # Move model to GPU\n",
        "loss_record = []  # To track progress\n",
        "num_epochs = 5\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  for batch in tqdm(train_loader):  # Use tqdm for progress bar\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlTUmVr37Z-Y"
      },
      "source": [
        "### 6.1 Visualizing Training Progress\n",
        "\n",
        "Let's plot the loss over time to see how our training progressed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8XxKaeOe6Es3"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(loss_record)\n",
        "plt.title('Training Loss')\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Binary Cross-Entropy Loss')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3rgGqTz7Z-b"
      },
      "source": [
        "## 7. Evaluating the Model\n",
        "\n",
        "Now that we've trained our model, let's evaluate it on the test set. We'll calculate metrics like precision and recall to see how well it identifies organizations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8w8-CIf98Gwz"
      },
      "outputs": [],
      "source": [
        "# Create a test dataset and loader\n",
        "testset = Dataset('test.txt', wrd2vec=wrd2vec)\n",
        "test_loader = DataLoader(testset, batch_size=200, collate_fn=custom_collate_fn)\n",
        "\n",
        "model.eval()  # Set the model to evaluation mode\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKB82rFc7Z-b"
      },
      "source": [
        "## 8. Using the Model for Inference\n",
        "\n",
        "Finally, let's use our trained model to predict whether words in a new sentence are organizations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lcnjh0QN6buk"
      },
      "outputs": [],
      "source": [
        "# Test on a new sentence\n",
        "test_str = \"students are studying in library\"\n",
        "test_words = test_str.split(' ')\n",
        "converted_vector = Dataset.get_flattened_vector(test_words, dataset.wrd2vec)\n",
        "converted_vector = torch.Tensor(converted_vector)\n",
        "prediction = model(converted_vector.to(dev))\n",
        "prediction  # Probability that the center word is an organization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbuGD2vn7Z-c"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "In this notebook, we've built a Named Entity Recognition system that identifies organizations in text. We:\n",
        "\n",
        "1. Processed the CoNLL2003 dataset to create context windows for each word\n",
        "2. Used pre-trained GloVe word embeddings to convert words to vectors\n",
        "3. Built a simple neural network classifier\n",
        "4. Trained the model on the training set\n",
        "5. Evaluated the model on the test set\n",
        "6. Used the model for inference on new text\n",
        "\n",
        "This model could be extended to identify other types of entities (people, locations, etc.) by modifying the labels. More advanced NER systems might use architectures like BiLSTMs or Transformers for better performance."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "nbformat": 4,
    "nbformat_minor": 5
  },
  "nbformat": 4,
  "nbformat_minor": 0
}